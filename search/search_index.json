{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-battery-intraday-trading-engine-documentation","title":"Welcome to the Battery Intraday Trading Engine Documentation!","text":"<p>This is a Python high-frequency intraday trading engine for simulating the rolling intrinsic strategy on the European market, solved as a dynamic program. See our paper pre-print (Arxiv) for details on the method and visualizations of the results. The code is published on GitHub.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Process your own raw EPEX Spot Market Data to a suitable format</li> <li>Define battery and simulation settings</li> <li>Adapt the parameters of the dynamic programming optimization</li> <li>React to every single LOB update on the exchange</li> <li>Run yearly high-frequency trading simulations in minutes/hours</li> <li>Return and visualize results and statistics</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>BitePy requires Python 3.8+ to run. The package is hosted on PyPI and can be easily installed via</p> <pre><code>pip install bitepy\n</code></pre>"},{"location":"#package","title":"Package","text":"<p>We divide our package into three major Python classes for preparing, running and visualizing the battery trading simulations. More detailed examples on how to use the package are given in the Tutorial.</p>"},{"location":"#data-preprocessing","title":"Data Preprocessing","text":"<p>Our <code>Data</code> class allows users to read-in raw zipped LOB Data from EPEX (2020 and later), process them accordingly and save each trading day as a separate CSV file. All Data is ultimately stored in UTC timezone format. We show and test this for German Market Data of the years 2020 and 2021, specifically using the 1h products of the continuous intraday market, but this can easily be adapted to other regions or other products. Inputs to the parsing function simply are the <code>start-day</code> and <code>end-day</code> of the data we want to parse, plus the <code>path</code> to the zipped EPEX market data.</p>"},{"location":"#simulation","title":"Simulation","text":"<p>The <code>Simulation</code> class enables users to initialize simulation instances, set parameters, load the preprocessed LOB Data into the simulation, run the simulation, and return results. Conceptually, you first set the parameters of the simulation (battery, dynamic programming, and simulation settings), then decide which days of LOB data to feed, before subsequently running the simulation for the desired amount of time. Order book traversals and optimizations happen in C++, while pre-/post-processing and settings are done in Python. Results are returned as Pandas dataframes and can be fed into the post-processing described below.</p>"},{"location":"#results-postprocessing","title":"Results Postprocessing","text":"<p>Our <code>Results</code> class, gives users some tools to visualize the final schedule, as determined by the rolling intrinsic simulation, and evaluate some key statistics. Of course, the user is encouraged to look at all simulation outputs in detail to understand the intricacies of the battery's trading behavior.</p>"},{"location":"#tutorial","title":"Tutorial","text":"<p>We give concrete usage examples and explanations to all the classes discussed above in our Jupyter Notebook.</p> <p>To reduce data-loading times, we encourage users to follow the flow of first creating LOB data CSV files with our <code>Data</code> class, but then creating LOB data binaries with our <code>Simulation</code> class, before running any simulations. Alternatively, users can also directly pass LOB Pandas dataframes to the simulation, at the cost of additional data-loading times.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#about-bitepy","title":"About BitePy","text":"<p>This Battery Intrade Trading Engine is the simulation code base for the paper (tbd) by Schaurecker et al. (2025). The base code is written in C++ and wrapped in Python for ease of use. Further features might be added in the near future, and this documentation will be updated accordingly.</p> <p>This code is written by David Schaurecker @dschaurecker, with pythonic support by Simon Hirsch @simon-hirsch, and is based on the work of his co-authors and himself. The Python code is open-source and can be used by anyone, but please cite the paper if you use it in your own work. We are happy to receive feedback and suggestions for improvement!</p>"},{"location":"data/","title":"Data Preprocessing","text":"<p>Our <code>Data</code> class allows users to read-in raw zipped LOB Data from EPEX (2020 and later), process them accordingly and save each trading day as a separate CSV file. All Data is ultimately stored in UTC timezone format. We show and test this for German Market Data of the years 2020 and 2021, specifically using the 1h products of the continuous intraday market, but this can easily be adapted to other regions or other products. Inputs to the parsing function simply are the <code>start-day</code> and <code>end-day</code> of the data we want to parse, plus the <code>path</code> to the zipped EPEX market data.</p> Source code in <code>bitepy/data.py</code> <pre><code>class Data:\n    def __init__(self):\n        \"\"\"Initialize a Data instance.\"\"\"\n        pass\n\n    def _load_csv(self, file_path):\n        \"\"\"\n        Load a single zipped CSV file with specified dtypes.\n        \"\"\"\n        df = pd.read_csv(\n            file_path,\n            compression=\"zip\",\n            dtype={\n                \"id\": np.int64,\n                \"initial\": np.int64,\n                \"side\": \"string\",\n                \"start\": \"string\",\n                \"transaction\": \"string\",\n                \"validity\": \"string\",\n                \"price\": np.float64,\n                \"quantity\": np.float64,\n            },\n        )\n        df.rename(columns={\"Unnamed: 0\": \"id\"}, inplace=True)\n        ids = df[\"id\"].to_numpy(dtype=np.int64).tolist()\n        initials = df[\"initial\"].to_numpy(dtype=np.int64).tolist()\n        sides = df[\"side\"].to_numpy(dtype=\"str\").tolist()\n        starts = df[\"start\"].to_numpy(dtype=\"str\").tolist()\n        transactions = df[\"transaction\"].to_numpy(dtype=\"str\").tolist()\n        validities = df[\"validity\"].to_numpy(dtype=\"str\").tolist()\n        prices = df[\"price\"].to_numpy(dtype=np.float64).tolist()\n        quantities = df[\"quantity\"].to_numpy(dtype=np.float64).tolist()\n        return ids, initials, sides, starts, transactions, validities, prices, quantities\n\n    def _read_id_table_2020(self, timestamp, datapath):\n        year = timestamp.strftime(\"%Y\")\n        month = timestamp.strftime(\"%m\")\n        datestr = \"Continuous_Orders_DE_\" + timestamp.strftime(\"%Y%m%d\")\n\n        # Get file name of zip-file and CSV file within the zip file\n        file_list = os.listdir(f\"{datapath}/{year}/{month}\")\n        zip_file_name = [i for i in file_list if datestr in i][0]\n        csv_file_name = zip_file_name[:-4]\n\n        # Read data from the CSV inside the zip file\n        zip_file = ZipFile(f\"{datapath}/{year}/{month}/\" + zip_file_name)\n        df = (pd.read_csv(zip_file.open(csv_file_name), sep=\";\", decimal=\".\")\n              .drop_duplicates(subset=[\"Order ID\", \"Initial ID\", \"Action code\", \"Validity time\", \"Price\", \"Quantity\"])\n              .loc[lambda x: x[\"Is User Defined Block\"] == 0]\n              .loc[lambda x: (x[\"Product\"] == \"Intraday_Hour_Power\") | (x[\"Product\"] == \"XBID_Hour_Power\")]\n              .loc[lambda x: (x[\"Action code\"] == \"A\") | (x[\"Action code\"] == \"D\") | (x[\"Action code\"] == \"C\") | (x[\"Action code\"] == \"I\")]\n              .drop([\"Delivery area\", \"Execution restriction\", \"Market area\", \"Parent ID\", \"Delivery End\",\n                     \"Currency\", \"Product\", \"isOTC\", \"Is User Defined Block\", \"Unnamed: 20\", \"RevisionNo\", \"Entry time\"],\n                    axis=1)\n              .rename({\"Order ID\": \"order\",\n                       \"Initial ID\": \"initial\",\n                       \"Delivery Start\": \"start\",\n                       \"Side\": \"side\",\n                       \"Price\": \"price\",\n                       \"Volume\": \"volume\",\n                       \"Validity time\": \"validity\",\n                       \"Action code\": \"action\",\n                       \"Transaction Time\": \"transaction\",\n                       \"Quantity\": \"quantity\"}, axis=1)\n              .assign(start=lambda x: pd.to_datetime(x.start, format=\"%Y-%m-%dT%H:%M:%SZ\"))\n              .assign(validity=lambda x: pd.to_datetime(x.validity, format=\"%Y-%m-%dT%H:%M:%SZ\"))\n              .assign(transaction=lambda x: pd.to_datetime(x.transaction, format=\"%Y-%m-%dT%H:%M:%S.%fZ\"))\n              )\n\n        # Remove iceberg orders\n        iceberg_IDs = df.loc[df[\"action\"] == \"I\", \"initial\"].unique()\n        df = df.loc[~df[\"initial\"].isin(iceberg_IDs)]\n\n        # Process change messages (action code 'C')\n        change_messages = df[df[\"action\"] == \"C\"].drop_duplicates(subset=[\"order\"], keep=\"first\")\n        not_added = change_messages[~(change_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n        change_messages = change_messages[~(change_messages[\"order\"].isin(not_added[\"order\"]))]\n\n        change_exists = change_messages.shape[0] &gt; 0\n        change_counter = 0\n        while change_exists:\n            indexer_messA_with_change = df[(df[\"order\"].isin(change_messages[\"order\"])) &amp; (df[\"action\"] == \"A\")] \\\n                .sort_values(\"transaction\").groupby(\"order\").tail(1).index\n\n            df[\"df_index_copy\"] = df.index\n            merged = pd.merge(change_messages, df.loc[indexer_messA_with_change], on='order')\n            df.loc[merged[\"df_index_copy\"].to_numpy(), \"validity\"] = merged[\"transaction_x\"].to_numpy()\n\n            # Change the action code from \"C\" to \"A\" for processed messages\n            df.loc[df.index.isin(change_messages.index), \"action\"] = \"A\"\n            df.drop(\"df_index_copy\", axis=1, inplace=True)\n\n            # Redo the procedure for remaining change messages\n            change_messages = df[df[\"action\"] == \"C\"].drop_duplicates(subset=[\"order\"], keep=\"first\")\n            not_added = change_messages[~(change_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n            change_messages = change_messages[~(change_messages[\"order\"].isin(not_added[\"order\"]))]\n            change_exists = change_messages.shape[0] &gt; 0\n            change_counter += 1\n\n        # Process cancel messages (action code 'D')\n        cancel_messages = df[df[\"action\"] == \"D\"]\n        not_added = cancel_messages[~(cancel_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n        cancel_messages = cancel_messages[~(cancel_messages[\"order\"].isin(not_added[\"order\"]))]\n\n        indexer_messA_with_cancel = df[(df[\"order\"].isin(cancel_messages[\"order\"])) &amp; (df[\"action\"] == \"A\")] \\\n            .sort_values(\"transaction\").groupby(\"order\").tail(1).index\n        df[\"df_index_copy\"] = df.index\n        merged = pd.merge(cancel_messages, df.loc[indexer_messA_with_cancel], on='order')\n        df.loc[merged[\"df_index_copy\"].to_numpy(), \"validity\"] = merged[\"transaction_x\"].to_numpy()\n\n        df = df.loc[lambda x: ~(x[\"action\"] == \"D\")]\n        df = df.drop([\"order\", \"action\", \"df_index_copy\"], axis=1)\n\n        # Reorder and format columns\n        newOrder = [\"initial\", \"side\", \"start\", \"transaction\", \"validity\", \"price\", \"quantity\"]\n        df = df[newOrder]\n        df['side'] = df['side'].str.upper()\n\n        df[\"start\"] = df[\"start\"].dt.tz_localize('UTC').dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n        df[\"transaction\"] = df[\"transaction\"].dt.tz_localize('UTC').dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n        df[\"validity\"] = df[\"validity\"].dt.tz_localize('UTC').dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n\n        return df\n\n    def _read_id_table_2021(self, timestamp, datapath):\n        year = timestamp.strftime(\"%Y\")\n        month = timestamp.strftime(\"%m\")\n        datestr = \"Continuous_Orders-DE-\" + timestamp.strftime(\"%Y%m%d\")\n\n        # Get file name of zip-file and CSV file within the zip file\n        file_list = os.listdir(f\"{datapath}/{year}/{month}\")\n        zip_file_name = [i for i in file_list if datestr in i][0]\n        csv_file_name = zip_file_name[:-4]\n\n        # Read data from the CSV inside the zip file\n        zip_file = ZipFile(f\"{datapath}/{year}/{month}/\" + zip_file_name)\n        df = (pd.read_csv(zip_file.open(csv_file_name), sep=\",\", decimal=\".\", skiprows=1)\n              .drop_duplicates(subset=[\"OrderId\", \"InitialId\", \"ActionCode\", \"ValidityTime\", \"Price\", \"Quantity\"])\n              .loc[lambda x: x[\"UserDefinedBlock\"] == \"N\"]\n              .loc[lambda x: (x[\"Product\"] == \"Intraday_Hour_Power\") | (x[\"Product\"] == \"XBID_Hour_Power\")]\n              .loc[lambda x: (x[\"ActionCode\"] == \"A\") | (x[\"ActionCode\"] == \"D\") | (x[\"ActionCode\"] == \"C\") | (x[\"ActionCode\"] == \"I\")]\n              .drop([\"LinkedBasketId\", \"DeliveryArea\", \"ParentId\", \"DeliveryEnd\", \"Currency\", \"Product\",\n                     \"UserDefinedBlock\", \"RevisionNo\", \"ExecutionRestriction\", \"CreationTime\", \"QuantityUnit\",\n                     \"Volume\", \"VolumeUnit\"], axis=1)\n              .rename({\"OrderId\": \"order\",\n                       \"InitialId\": \"initial\",\n                       \"DeliveryStart\": \"start\",\n                       \"Side\": \"side\",\n                       \"Price\": \"price\",\n                       \"Volume\": \"volume\",\n                       \"ValidityTime\": \"validity\",\n                       \"ActionCode\": \"action\",\n                       \"TransactionTime\": \"transaction\",\n                       \"Quantity\": \"quantity\"}, axis=1)\n              .assign(start=lambda x: pd.to_datetime(x.start, format=\"%Y-%m-%dT%H:%M:%SZ\"))\n              .assign(validity=lambda x: pd.to_datetime(x.validity, format=\"%Y-%m-%dT%H:%M:%SZ\"))\n              .assign(transaction=lambda x: pd.to_datetime(x.transaction, format=\"%Y-%m-%dT%H:%M:%S.%fZ\"))\n              )\n        # Remove iceberg orders\n        iceberg_IDs = df.loc[df[\"action\"] == \"I\", \"initial\"].unique()\n        df = df.loc[~df[\"initial\"].isin(iceberg_IDs)]\n\n        # Process change messages (action code 'C')\n        change_messages = df[df[\"action\"] == \"C\"].drop_duplicates(subset=[\"order\"], keep=\"first\")\n        not_added = change_messages[~(change_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n        change_messages = change_messages[~(change_messages[\"order\"].isin(not_added[\"order\"]))]\n\n        change_exists = change_messages.shape[0] &gt; 0\n        change_counter = 0\n        while change_exists:\n            indexer_messA_with_change = df[(df[\"order\"].isin(change_messages[\"order\"])) &amp; (df[\"action\"] == \"A\")] \\\n                .sort_values(\"transaction\").groupby(\"order\").tail(1).index\n\n            df[\"df_index_copy\"] = df.index\n            merged = pd.merge(change_messages, df.loc[indexer_messA_with_change], on='order')\n            df.loc[merged[\"df_index_copy\"].to_numpy(), \"validity\"] = merged[\"transaction_x\"].to_numpy()\n\n            # Change the action code from \"C\" to \"A\" so it can be processed in the next iteration\n            df.loc[df.index.isin(change_messages.index), \"action\"] = \"A\"\n            df.drop(\"df_index_copy\", axis=1, inplace=True)\n\n            # Redo procedure for remaining change messages\n            change_messages = df[df[\"action\"] == \"C\"].drop_duplicates(subset=[\"order\"], keep=\"first\")\n            not_added = change_messages[~(change_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n            change_messages = change_messages[~(change_messages[\"order\"].isin(not_added[\"order\"]))]\n            change_exists = change_messages.shape[0] &gt; 0\n            change_counter += 1\n\n        # Process cancel messages (action code 'D')\n        cancel_messages = df[df[\"action\"] == \"D\"]\n        not_added = cancel_messages[~(cancel_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n        cancel_messages = cancel_messages[~(cancel_messages[\"order\"].isin(not_added[\"order\"]))]\n\n        indexer_messA_with_cancel = df[(df[\"order\"].isin(cancel_messages[\"order\"])) &amp; (df[\"action\"] == \"A\")] \\\n            .sort_values(\"transaction\").groupby(\"order\").tail(1).index\n        df[\"df_index_copy\"] = df.index\n        merged = pd.merge(cancel_messages, df.loc[indexer_messA_with_cancel], on='order')\n        df.loc[merged[\"df_index_copy\"].to_numpy(), \"validity\"] = merged[\"transaction_x\"].to_numpy()\n\n        df = df.loc[lambda x: ~(x[\"action\"] == \"D\")]\n        df = df.drop([\"order\", \"action\", \"df_index_copy\"], axis=1)\n\n        df[\"start\"] = df[\"start\"].dt.tz_localize('UTC').dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n        df[\"transaction\"] = df[\"transaction\"].dt.tz_localize('UTC').dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n        df[\"validity\"] = df[\"validity\"].dt.tz_localize('UTC').dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n\n        return df\n\n\n    def _read_nordpool_table(self, date, marketdatapath):\n        \"\"\"Read and process NordPool parquet files for a specific date.\n           Nordpool contains flags for full and partial execution of orders. We disregard this, as it will become apparent in our backtesting LOB traversal. After partial execution, orders are sometimes modified, deleted etc., this all stays relevant and is handled.\n           We also currently still disregard FoK and IoC orders (treat them as 0 validity duration). They have all the same updateTime in their message-chain.\n        \"\"\"\n        date_folder = date.strftime(\"%Y%m%d\")\n        folder_path = Path(marketdatapath) / date_folder\n\n        if not folder_path.exists():\n            raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n\n        parquet_files = sorted(folder_path.glob(\"NordPool_*.parquet\"))\n\n        if not parquet_files:\n            raise FileNotFoundError(f\"No parquet files found in folder: {folder_path}\")\n\n        # Read and concatenate all hourly parquet files\n        dfs = []\n        for file in parquet_files:\n            df_temp = pd.read_parquet(file)\n            dfs.append(df_temp)\n\n        df = pd.concat(dfs, ignore_index=True)\n\n        # Convert timestamps (needed for subsequent filtering and processing)\n        df = (df\n            .assign(createdTime=lambda x: pd.to_datetime(x['createdTime'], format='ISO8601'))\n            .assign(updatedTime=lambda x: pd.to_datetime(x['updatedTime'], format='ISO8601'))\n            .assign(expirationTime=lambda x: pd.to_datetime(x['expirationTime'], format='ISO8601'))\n            .assign(deliveryStart=lambda x: pd.to_datetime(x['deliveryStart'], format='ISO8601'))\n            .assign(deliveryEnd=lambda x: pd.to_datetime(x['deliveryEnd'], format='ISO8601'))\n            )\n\n        # Filter and prepare data\n        df = (df\n            .drop_duplicates(subset=['orderId', 'originalOrderId', 'action', 'expirationTime', 'price', 'volume'])\n            .loc[lambda x: x['contractName'].str.startswith('PH')]\n            .loc[lambda x: x['action'].isin(['UserAdded', 'UserModified', 'UserDeleted', 'SystemDeleted', 'UserHibernated'])]\n            )\n\n        # Remove iceberg orders\n        iceberg_IDs = df.loc[df['orderType'] == 'Iceberg', 'originalOrderId'].unique()\n        df = df.loc[~df['originalOrderId'].isin(iceberg_IDs)]\n\n        # Replace letters with numbers in originalOrderId and orderId\n        unique_letters = sorted(df['originalOrderId'].astype(str).str.findall(r'[A-Za-z]').str.join('').unique())\n        # Create mapping of unique letters to numbers starting from 11\n        letter_to_num = {letter: str(i+11) for i, letter in enumerate(unique_letters)}\n        # Function to replace letters with numbers\n        def replace_letters(order_id):\n            order_id = str(order_id)\n            for letter, num in letter_to_num.items():\n                order_id = order_id.replace(letter, num)\n            return order_id\n\n        # Apply replacement to originalOrderId column\n        df['originalOrderId'] = df['originalOrderId'].apply(replace_letters)\n        df['orderId'] = df['orderId'].apply(replace_letters)\n\n        # Rename columns to standardized format\n        df = df.rename(columns={\n            'orderId': 'order',\n            'originalOrderId': 'initial',\n            'deliveryStart': 'start',\n            'updatedTime': 'transaction',\n            'expirationTime': 'validity',\n            'volume': 'quantity',\n            'action': 'action_original'\n        })\n\n        # Map NordPool actions to standardized codes\n        df['action'] = df['action_original'].map({\n            'UserAdded': 'A',\n            'UserModified': 'C',\n            'UserDeleted': 'D',\n            'SystemDeleted': 'D',\n            'UserHibernated': 'H'\n        })\n\n        # Process change messages (modifications)\n        change_messages = df[df[\"action\"] == \"C\"].drop_duplicates(subset=[\"order\"], keep=\"first\")\n        not_added = change_messages[~(change_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n        change_messages = change_messages[~(change_messages[\"order\"].isin(not_added[\"order\"]))]\n\n        change_exists = change_messages.shape[0] &gt; 0\n        while change_exists:\n            indexer_messA_with_change = df[(df[\"order\"].isin(change_messages[\"order\"])) &amp; (df[\"action\"] == \"A\")] \\\n                .sort_values(\"transaction\").groupby(\"order\").tail(1).index\n            df[\"df_index_copy\"] = df.index\n            merged = pd.merge(change_messages, df.loc[indexer_messA_with_change], on='order')\n            df.loc[merged[\"df_index_copy\"].to_numpy(), \"validity\"] = merged[\"transaction_x\"].to_numpy()\n            df.loc[df.index.isin(change_messages.index), \"action\"] = \"A\"\n            df.drop(\"df_index_copy\", axis=1, inplace=True)\n\n            change_messages = df[df[\"action\"] == \"C\"].drop_duplicates(subset=[\"order\"], keep=\"first\")\n            not_added = change_messages[~(change_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n            change_messages = change_messages[~(change_messages[\"order\"].isin(not_added[\"order\"]))]\n            change_exists = change_messages.shape[0] &gt; 0\n\n        # Process cancel messages (deletions)\n        cancel_messages = df[df[\"action\"] == \"D\"]\n        not_added = cancel_messages[~(cancel_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n        cancel_messages = cancel_messages[~(cancel_messages[\"order\"].isin(not_added[\"order\"]))]\n\n        indexer_messA_with_cancel = df[(df[\"order\"].isin(cancel_messages[\"order\"])) &amp; (df[\"action\"] == \"A\")] \\\n            .sort_values(\"transaction\").groupby(\"order\").tail(1).index\n        df[\"df_index_copy\"] = df.index\n        merged = pd.merge(cancel_messages, df.loc[indexer_messA_with_cancel], on='order')\n        df.loc[merged[\"df_index_copy\"].to_numpy(), \"validity\"] = merged[\"transaction_x\"].to_numpy()\n        df.drop(\"df_index_copy\", axis=1, inplace=True)\n\n        df = df.loc[lambda x: ~(x[\"action\"] == \"D\")]\n\n        # Process hibernation messages\n        hibernated_messages = df[df[\"action\"] == \"H\"]\n        not_added = hibernated_messages[~(hibernated_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n        hibernated_messages = hibernated_messages[~(hibernated_messages[\"order\"].isin(not_added[\"order\"]))]\n\n        if not hibernated_messages.empty:\n            indexer_messA_with_hibernated = df[(df[\"order\"].isin(hibernated_messages[\"order\"])) &amp; (df[\"action\"] == \"A\")] \\\n                .sort_values(\"transaction\").groupby(\"order\").tail(1).index\n            df[\"df_index_copy\"] = df.index\n            merged = pd.merge(hibernated_messages, df.loc[indexer_messA_with_hibernated], on='order')\n            df.loc[merged[\"df_index_copy\"].to_numpy(), \"validity\"] = merged[\"transaction_x\"].to_numpy()\n            df.drop(\"df_index_copy\", axis=1, inplace=True)\n\n        df = df.loc[lambda x: ~(x[\"action\"] == \"H\")]\n        df = df.drop([\"order\", \"action\", \"action_original\"], axis=1, errors='ignore')\n\n        # Filter out orders where validity time is not after transaction time; Sometimes orders are added and deleted at the same time.\n        df = df[df['validity'] &gt; df['transaction']]\n\n        # Convert timestamps to string format\n        df[\"start\"] = df[\"start\"].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n        df[\"transaction\"] = df[\"transaction\"].dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n        df[\"validity\"] = df[\"validity\"].dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n\n        # rename side to all uppercase\n        df['side'] = df['side'].str.upper()\n\n        # Select and order final columns\n        df = df[['initial', 'side', 'start', 'transaction', 'validity', 'price', 'quantity']]\n\n        return df\n\n\n    def parse_market_data(self, start_date_str: str, end_date_str: str, marketdatapath: str, \n                        savepath: str, market_type: str, verbose: bool = True):\n        \"\"\"\n        Parse market data between two dates and save processed zipped CSV files.\n\n        Processes raw order book data from EPEX or NordPool markets and converts them into \n        standardized sorted CSV files for each day in UTC time format. Handles order lifecycle \n        events (additions, modifications, cancellations) and reconstructs order validity periods.\n\n        Args:\n            start_date_str (str): Start date in format \"YYYY-MM-DD\"\n            end_date_str (str): End date in format \"YYYY-MM-DD\"\n            marketdatapath (str): Path to market data folder with yearly/monthly subfolders\n            savepath (str): Directory where processed CSV files will be saved\n            market_type (str): \"EPEX\" or \"NordPool\"\n            verbose (bool, optional): Print progress messages. Defaults to True.\n        \"\"\"\n\n        if not os.path.exists(savepath):\n            os.makedirs(savepath)\n\n        start_date = pd.Timestamp(start_date_str)\n        end_date = pd.Timestamp(end_date_str)\n\n        if start_date &gt; end_date:\n            raise ValueError(\"Error: Start date is after end date.\")\n        if market_type == \"EPEX\" and start_date.year &lt; 2020:\n            raise ValueError(\"Error: Years before 2020 are not supported.\")\n\n        dates = pd.date_range(start_date, end_date, freq=\"D\")\n        df1 = pd.DataFrame()\n        df2 = pd.DataFrame()\n\n        with tqdm(total=len(dates), desc=\"Loading and saving CSV data\", ncols=100, disable=not verbose) as pbar:\n            for dt1 in dates:\n                pbar.set_description(f\"Currently loading and saving date {str(dt1.date())} ... \")\n                df1 = df2\n                df2 = pd.DataFrame()\n                dt2 = dt1 + pd.Timedelta(days=1)\n\n                # Read current day data\n                if df1.empty:\n                    if market_type == \"EPEX\":\n                        if dt1.year == 2020:\n                            df1 = self._read_id_table_2020(dt1, marketdatapath)\n                        elif dt1.year &gt;= 2021:\n                            df1 = self._read_id_table_2021(dt1, marketdatapath)\n                        else:\n                            raise ValueError(\"Error: Year not &gt;= 2020\")\n                    elif market_type == \"NordPool\":\n                        df1 = self._read_nordpool_table(dt1, marketdatapath)\n                    else:\n                        raise ValueError(f\"Unknown market_type: {market_type}\")\n\n                # Read next day data (captures orders with transaction today, delivery tomorrow)\n                if dt2 &lt;= end_date:\n                    if market_type == \"EPEX\":\n                        if dt2.year == 2020:\n                            df2 = self._read_id_table_2020(dt2, marketdatapath)\n                        elif dt2.year &gt;= 2021:\n                            df2 = self._read_id_table_2021(dt2, marketdatapath)\n                        else:\n                            raise ValueError(\"Error: Year not &gt;= 2020\")\n                    elif market_type == \"NordPool\":\n                        df2 = self._read_nordpool_table(dt2, marketdatapath)\n                    else:\n                        raise ValueError(f\"Unknown market_type: {market_type}\")\n\n                # Combine and filter by transaction date\n                df = pd.concat([df1, df2])\n                df = df.sort_values(by='transaction')\n                df['transaction_date'] = pd.to_datetime(df['transaction']).dt.date\n                grouped = df.groupby('transaction_date')\n\n                # round price to 2 decimals and quantity to 1 decimal\n                df['price'] = df['price'].round(2)\n                df['quantity'] = df['quantity'].round(1)\n\n                save_date = dt1.date()\n                group = grouped.get_group(save_date)\n                daily_filename = f\"{savepath}orderbook_{save_date}.csv\"\n                compression_options = dict(method='zip', archive_name=Path(daily_filename).name)\n                group.drop(columns='transaction_date').sort_values(by='transaction').fillna(\"\").to_csv(\n                    f'{daily_filename}.zip', compression=compression_options)\n                pbar.update(1)\n\n        print(\"\\nWriting CSV data completed.\")\n\n    def create_bins_from_csv(self, csv_list: list, save_path: str, verbose: bool = True):\n        \"\"\"\n        Convert zipped CSV files of pre-processed order book data into binary files.\n\n        This method sequentially loads each previously generated zipped CSV file, converts it to a binary format using the C++ simulation\n        extension, and saves the binary file in the specified directory. Binary files allow for much (10x) quicker loading\n        of the data at runtime.\n\n        Args:\n            csv_list (list): List of file paths to the zipped CSV files containing pre-processed order book data.\n            save_path (str): Directory path where the binary files should be saved. The binary files will use the same base name as the CSV files.\n            verbose (bool, optional): If True, print progress messages. Defaults to True.\n        \"\"\"\n        if not os.path.exists(save_path):\n            os.makedirs(save_path)\n\n        _sim = Simulation_cpp()\n        with tqdm(total=len(csv_list), desc=\"Writing Binaries\", ncols=100, disable=not verbose) as pbar:\n            for csv_file_path in csv_list:\n                filename = os.path.basename(csv_file_path)\n                bin_file_path = os.path.join(save_path, filename.replace(\".csv.zip\", \".bin\"))\n                pbar.set_description(f\"Currently saving binary {bin_file_path.split('/')[-1]} ... \")\n                ids, initials, sides, starts, transactions, validities, prices, quantities = self._load_csv(csv_file_path)\n                _sim.writeOrderBinFromPandas(\n                    bin_file_path,\n                    ids,\n                    initials,\n                    sides,\n                    starts,\n                    transactions,\n                    validities,\n                    prices,\n                    quantities,\n                )\n                pbar.update(1)\n\n        print(\"\\nWriting Binaries completed.\")\n</code></pre>"},{"location":"data/#bitepy.Data.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a Data instance.</p> Source code in <code>bitepy/data.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize a Data instance.\"\"\"\n    pass\n</code></pre>"},{"location":"data/#bitepy.Data.create_bins_from_csv","title":"<code>create_bins_from_csv(csv_list, save_path, verbose=True)</code>","text":"<p>Convert zipped CSV files of pre-processed order book data into binary files.</p> <p>This method sequentially loads each previously generated zipped CSV file, converts it to a binary format using the C++ simulation extension, and saves the binary file in the specified directory. Binary files allow for much (10x) quicker loading of the data at runtime.</p> <p>Parameters:</p> Name Type Description Default <code>csv_list</code> <code>list</code> <p>List of file paths to the zipped CSV files containing pre-processed order book data.</p> required <code>save_path</code> <code>str</code> <p>Directory path where the binary files should be saved. The binary files will use the same base name as the CSV files.</p> required <code>verbose</code> <code>bool</code> <p>If True, print progress messages. Defaults to True.</p> <code>True</code> Source code in <code>bitepy/data.py</code> <pre><code>def create_bins_from_csv(self, csv_list: list, save_path: str, verbose: bool = True):\n    \"\"\"\n    Convert zipped CSV files of pre-processed order book data into binary files.\n\n    This method sequentially loads each previously generated zipped CSV file, converts it to a binary format using the C++ simulation\n    extension, and saves the binary file in the specified directory. Binary files allow for much (10x) quicker loading\n    of the data at runtime.\n\n    Args:\n        csv_list (list): List of file paths to the zipped CSV files containing pre-processed order book data.\n        save_path (str): Directory path where the binary files should be saved. The binary files will use the same base name as the CSV files.\n        verbose (bool, optional): If True, print progress messages. Defaults to True.\n    \"\"\"\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n\n    _sim = Simulation_cpp()\n    with tqdm(total=len(csv_list), desc=\"Writing Binaries\", ncols=100, disable=not verbose) as pbar:\n        for csv_file_path in csv_list:\n            filename = os.path.basename(csv_file_path)\n            bin_file_path = os.path.join(save_path, filename.replace(\".csv.zip\", \".bin\"))\n            pbar.set_description(f\"Currently saving binary {bin_file_path.split('/')[-1]} ... \")\n            ids, initials, sides, starts, transactions, validities, prices, quantities = self._load_csv(csv_file_path)\n            _sim.writeOrderBinFromPandas(\n                bin_file_path,\n                ids,\n                initials,\n                sides,\n                starts,\n                transactions,\n                validities,\n                prices,\n                quantities,\n            )\n            pbar.update(1)\n\n    print(\"\\nWriting Binaries completed.\")\n</code></pre>"},{"location":"data/#bitepy.Data.parse_market_data","title":"<code>parse_market_data(start_date_str, end_date_str, marketdatapath, savepath, market_type, verbose=True)</code>","text":"<p>Parse market data between two dates and save processed zipped CSV files.</p> <p>Processes raw order book data from EPEX or NordPool markets and converts them into  standardized sorted CSV files for each day in UTC time format. Handles order lifecycle  events (additions, modifications, cancellations) and reconstructs order validity periods.</p> <p>Parameters:</p> Name Type Description Default <code>start_date_str</code> <code>str</code> <p>Start date in format \"YYYY-MM-DD\"</p> required <code>end_date_str</code> <code>str</code> <p>End date in format \"YYYY-MM-DD\"</p> required <code>marketdatapath</code> <code>str</code> <p>Path to market data folder with yearly/monthly subfolders</p> required <code>savepath</code> <code>str</code> <p>Directory where processed CSV files will be saved</p> required <code>market_type</code> <code>str</code> <p>\"EPEX\" or \"NordPool\"</p> required <code>verbose</code> <code>bool</code> <p>Print progress messages. Defaults to True.</p> <code>True</code> Source code in <code>bitepy/data.py</code> <pre><code>def parse_market_data(self, start_date_str: str, end_date_str: str, marketdatapath: str, \n                    savepath: str, market_type: str, verbose: bool = True):\n    \"\"\"\n    Parse market data between two dates and save processed zipped CSV files.\n\n    Processes raw order book data from EPEX or NordPool markets and converts them into \n    standardized sorted CSV files for each day in UTC time format. Handles order lifecycle \n    events (additions, modifications, cancellations) and reconstructs order validity periods.\n\n    Args:\n        start_date_str (str): Start date in format \"YYYY-MM-DD\"\n        end_date_str (str): End date in format \"YYYY-MM-DD\"\n        marketdatapath (str): Path to market data folder with yearly/monthly subfolders\n        savepath (str): Directory where processed CSV files will be saved\n        market_type (str): \"EPEX\" or \"NordPool\"\n        verbose (bool, optional): Print progress messages. Defaults to True.\n    \"\"\"\n\n    if not os.path.exists(savepath):\n        os.makedirs(savepath)\n\n    start_date = pd.Timestamp(start_date_str)\n    end_date = pd.Timestamp(end_date_str)\n\n    if start_date &gt; end_date:\n        raise ValueError(\"Error: Start date is after end date.\")\n    if market_type == \"EPEX\" and start_date.year &lt; 2020:\n        raise ValueError(\"Error: Years before 2020 are not supported.\")\n\n    dates = pd.date_range(start_date, end_date, freq=\"D\")\n    df1 = pd.DataFrame()\n    df2 = pd.DataFrame()\n\n    with tqdm(total=len(dates), desc=\"Loading and saving CSV data\", ncols=100, disable=not verbose) as pbar:\n        for dt1 in dates:\n            pbar.set_description(f\"Currently loading and saving date {str(dt1.date())} ... \")\n            df1 = df2\n            df2 = pd.DataFrame()\n            dt2 = dt1 + pd.Timedelta(days=1)\n\n            # Read current day data\n            if df1.empty:\n                if market_type == \"EPEX\":\n                    if dt1.year == 2020:\n                        df1 = self._read_id_table_2020(dt1, marketdatapath)\n                    elif dt1.year &gt;= 2021:\n                        df1 = self._read_id_table_2021(dt1, marketdatapath)\n                    else:\n                        raise ValueError(\"Error: Year not &gt;= 2020\")\n                elif market_type == \"NordPool\":\n                    df1 = self._read_nordpool_table(dt1, marketdatapath)\n                else:\n                    raise ValueError(f\"Unknown market_type: {market_type}\")\n\n            # Read next day data (captures orders with transaction today, delivery tomorrow)\n            if dt2 &lt;= end_date:\n                if market_type == \"EPEX\":\n                    if dt2.year == 2020:\n                        df2 = self._read_id_table_2020(dt2, marketdatapath)\n                    elif dt2.year &gt;= 2021:\n                        df2 = self._read_id_table_2021(dt2, marketdatapath)\n                    else:\n                        raise ValueError(\"Error: Year not &gt;= 2020\")\n                elif market_type == \"NordPool\":\n                    df2 = self._read_nordpool_table(dt2, marketdatapath)\n                else:\n                    raise ValueError(f\"Unknown market_type: {market_type}\")\n\n            # Combine and filter by transaction date\n            df = pd.concat([df1, df2])\n            df = df.sort_values(by='transaction')\n            df['transaction_date'] = pd.to_datetime(df['transaction']).dt.date\n            grouped = df.groupby('transaction_date')\n\n            # round price to 2 decimals and quantity to 1 decimal\n            df['price'] = df['price'].round(2)\n            df['quantity'] = df['quantity'].round(1)\n\n            save_date = dt1.date()\n            group = grouped.get_group(save_date)\n            daily_filename = f\"{savepath}orderbook_{save_date}.csv\"\n            compression_options = dict(method='zip', archive_name=Path(daily_filename).name)\n            group.drop(columns='transaction_date').sort_values(by='transaction').fillna(\"\").to_csv(\n                f'{daily_filename}.zip', compression=compression_options)\n            pbar.update(1)\n\n    print(\"\\nWriting CSV data completed.\")\n</code></pre>"},{"location":"results/","title":"Results Postprocessing","text":"<p>Our <code>Results</code> class, gives users some tools to visualize the final schedule, as determined by the rolling intrinsic simulation, and evaluate some key statistics. Of course, the user is encouraged to look at all simulation outputs in detail to understand the intricacies of the battery's trading behavior.</p> Source code in <code>bitepy/results.py</code> <pre><code>class Results:\n    def __init__(self, logs: dict):\n        \"\"\"\n        Initialize a Simulation instance.\n\n        Args:\n            logs (dict): A dictionary containing the get_logs() output of the simulation class.\n        \"\"\"\n\n        self.logs = logs\n\n    def get_total_reward(self):\n        return np.round(self.logs[\"decision_record\"]['full_reward'].sum(),2)\n\n    def plot_decision_chart(self,lleft: int = 0,lright: int = -1):\n        \"\"\"\n        Plot the storage, market-position, and reward of the agent over the selected simulation period.\n\n        Args:\n            lleft (int): The left index of the simulation period.\n            lright (int): The right index of the simulation period.\n        \"\"\"\n        df = self.logs[\"decision_record\"]\n\n        # plot storage, position, and reward where reward is in a seperate axis below\n        fig1, ax1 = plt.subplots(figsize=(18, 10))\n        ax2 = ax1.twinx()\n        ax1.plot(df[\"hour\"][lleft:lright], df['storage'][lleft:lright], color='blue')\n        #plot position as points not line\n        ax1.plot(df[\"hour\"][lleft:lright], df['position'][lleft:lright], 'o', color='red')\n\n\n        ax2.plot(df[\"hour\"][lleft:lright], df['full_reward'][lleft:lright], color='green')\n        ax1.set_xlabel('Time ')\n        ax1.set_ylabel('Storage (MWh)')\n        ax2.set_ylabel('Reward (\u20ac)')\n        # plot a horizontal line at 0\n        ax2.axhline(y=0, color='green', linestyle='--', linewidth=1.5, alpha=0.5)\n        ax1.legend(['storage', 'position'], loc='upper left')\n        ax2.legend(['reward'], loc='upper right')\n        #set gridlines\n        # plot vertical grid line for each hour\n\n        # # array for each hour between lleft and lright\n        # hours = pd.date_range(start=df[\"hour\"].iloc[lleft], end=df[\"hour\"].iloc[lright], freq='D')\n        # for hour in hours:\n        #     ax1.axvline(x=hour, color='gray', linestyle='--', linewidth=0.5)\n\n        ax1.grid(True, alpha=0.5)\n        plt.show()\n\n        fig1, ax1 = plt.subplots(figsize=(18, 10))\n        #plot cumulative reward\n        ax1.plot(df[\"hour\"][lleft:lright], df['full_reward'].cumsum()[lleft:lright], color='blue')\n        ax1.set_xlabel('Time ')\n        ax1.set_ylabel('Cumulative Reward (\u20ac)')\n        ax1.grid(True, alpha=0.5)\n        plt.show()\n\n    def plot_heatmap(self):\n        \"\"\"\n        Plot a heatmap of the final storage positions and visualize the executed orders over the simulation period.\n        Heatmap plots adapted from: https://github.com/bitstoenergy/iclr-smartmeteranalytics by Markus Kreft.\n        \"\"\"\n\n        df = self.logs[\"decision_record\"]\n        df.index = df[\"hour\"]\n        df = df.drop(columns=[\"hour\"])\n\n        exec_df = self.logs[\"executed_orders\"].drop(columns=[\"dp_run\", \"last_solve_time\", \"final_pos\", \"final_stor\"])\n        exec_df.set_index('hour', inplace=True)\n\n        # Create an empty list to store results\n        daily_volumes = []\n\n        # Iterate through unique dates in df\n        for day in np.unique(df.index.date):\n            # Filter the data for the current day\n            daily_data = exec_df.loc[exec_df.index.date == day]\n\n            # Calculate the largest daily volume and the summed daily volume\n            largest_daily_vol = daily_data[\"volume\"].max()\n            summed_daily_vol = daily_data[\"volume\"].sum()\n\n            # Append the results as a dictionary\n            daily_volumes.append({\n                \"date\": day,\n                \"max_vol\": largest_daily_vol,\n                \"summed_vol\": summed_daily_vol\n            })\n\n        # Create a DataFrame from the list of dictionaries\n        daily_volumes_df = pd.DataFrame(daily_volumes)\n\n        # Set the 'date' column as the index\n        daily_volumes_df.set_index(\"date\", inplace=True)\n        daily_volumes_df.index = pd.to_datetime(daily_volumes_df.index).tz_localize(\"UTC\")\n\n\n        fig = hm.HeatmapFigure(df, daily_volumes_df, 'storage', interval_minutes=60, figsize=(14, 8))\n        plt.show()\n</code></pre>"},{"location":"results/#bitepy.Results.__init__","title":"<code>__init__(logs)</code>","text":"<p>Initialize a Simulation instance.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>dict</code> <p>A dictionary containing the get_logs() output of the simulation class.</p> required Source code in <code>bitepy/results.py</code> <pre><code>def __init__(self, logs: dict):\n    \"\"\"\n    Initialize a Simulation instance.\n\n    Args:\n        logs (dict): A dictionary containing the get_logs() output of the simulation class.\n    \"\"\"\n\n    self.logs = logs\n</code></pre>"},{"location":"results/#bitepy.Results.plot_decision_chart","title":"<code>plot_decision_chart(lleft=0, lright=-1)</code>","text":"<p>Plot the storage, market-position, and reward of the agent over the selected simulation period.</p> <p>Parameters:</p> Name Type Description Default <code>lleft</code> <code>int</code> <p>The left index of the simulation period.</p> <code>0</code> <code>lright</code> <code>int</code> <p>The right index of the simulation period.</p> <code>-1</code> Source code in <code>bitepy/results.py</code> <pre><code>def plot_decision_chart(self,lleft: int = 0,lright: int = -1):\n    \"\"\"\n    Plot the storage, market-position, and reward of the agent over the selected simulation period.\n\n    Args:\n        lleft (int): The left index of the simulation period.\n        lright (int): The right index of the simulation period.\n    \"\"\"\n    df = self.logs[\"decision_record\"]\n\n    # plot storage, position, and reward where reward is in a seperate axis below\n    fig1, ax1 = plt.subplots(figsize=(18, 10))\n    ax2 = ax1.twinx()\n    ax1.plot(df[\"hour\"][lleft:lright], df['storage'][lleft:lright], color='blue')\n    #plot position as points not line\n    ax1.plot(df[\"hour\"][lleft:lright], df['position'][lleft:lright], 'o', color='red')\n\n\n    ax2.plot(df[\"hour\"][lleft:lright], df['full_reward'][lleft:lright], color='green')\n    ax1.set_xlabel('Time ')\n    ax1.set_ylabel('Storage (MWh)')\n    ax2.set_ylabel('Reward (\u20ac)')\n    # plot a horizontal line at 0\n    ax2.axhline(y=0, color='green', linestyle='--', linewidth=1.5, alpha=0.5)\n    ax1.legend(['storage', 'position'], loc='upper left')\n    ax2.legend(['reward'], loc='upper right')\n    #set gridlines\n    # plot vertical grid line for each hour\n\n    # # array for each hour between lleft and lright\n    # hours = pd.date_range(start=df[\"hour\"].iloc[lleft], end=df[\"hour\"].iloc[lright], freq='D')\n    # for hour in hours:\n    #     ax1.axvline(x=hour, color='gray', linestyle='--', linewidth=0.5)\n\n    ax1.grid(True, alpha=0.5)\n    plt.show()\n\n    fig1, ax1 = plt.subplots(figsize=(18, 10))\n    #plot cumulative reward\n    ax1.plot(df[\"hour\"][lleft:lright], df['full_reward'].cumsum()[lleft:lright], color='blue')\n    ax1.set_xlabel('Time ')\n    ax1.set_ylabel('Cumulative Reward (\u20ac)')\n    ax1.grid(True, alpha=0.5)\n    plt.show()\n</code></pre>"},{"location":"results/#bitepy.Results.plot_heatmap","title":"<code>plot_heatmap()</code>","text":"<p>Plot a heatmap of the final storage positions and visualize the executed orders over the simulation period. Heatmap plots adapted from: https://github.com/bitstoenergy/iclr-smartmeteranalytics by Markus Kreft.</p> Source code in <code>bitepy/results.py</code> <pre><code>def plot_heatmap(self):\n    \"\"\"\n    Plot a heatmap of the final storage positions and visualize the executed orders over the simulation period.\n    Heatmap plots adapted from: https://github.com/bitstoenergy/iclr-smartmeteranalytics by Markus Kreft.\n    \"\"\"\n\n    df = self.logs[\"decision_record\"]\n    df.index = df[\"hour\"]\n    df = df.drop(columns=[\"hour\"])\n\n    exec_df = self.logs[\"executed_orders\"].drop(columns=[\"dp_run\", \"last_solve_time\", \"final_pos\", \"final_stor\"])\n    exec_df.set_index('hour', inplace=True)\n\n    # Create an empty list to store results\n    daily_volumes = []\n\n    # Iterate through unique dates in df\n    for day in np.unique(df.index.date):\n        # Filter the data for the current day\n        daily_data = exec_df.loc[exec_df.index.date == day]\n\n        # Calculate the largest daily volume and the summed daily volume\n        largest_daily_vol = daily_data[\"volume\"].max()\n        summed_daily_vol = daily_data[\"volume\"].sum()\n\n        # Append the results as a dictionary\n        daily_volumes.append({\n            \"date\": day,\n            \"max_vol\": largest_daily_vol,\n            \"summed_vol\": summed_daily_vol\n        })\n\n    # Create a DataFrame from the list of dictionaries\n    daily_volumes_df = pd.DataFrame(daily_volumes)\n\n    # Set the 'date' column as the index\n    daily_volumes_df.set_index(\"date\", inplace=True)\n    daily_volumes_df.index = pd.to_datetime(daily_volumes_df.index).tz_localize(\"UTC\")\n\n\n    fig = hm.HeatmapFigure(df, daily_volumes_df, 'storage', interval_minutes=60, figsize=(14, 8))\n    plt.show()\n</code></pre>"},{"location":"simulation/","title":"Simulation","text":"<p>The <code>Simulation</code> class enables users to initialize simulation instances, set parameters, load the preprocessed LOB Data into the simulation, run the simulation, and return results. Conceptually, you first set the parameters of the simulation (battery, dynamic programming, and simulation settings), then decide which days of LOB data to feed, before subsequently running the simulation for the desired amount of time. Order book traversals and optimizations happen in C++, while pre-/post-processing and settings are done in Python. Results are returned as Pandas dataframes and can be fed into the post-processing described below.</p> Source code in <code>bitepy/simulation.py</code> <pre><code>class Simulation:\n    def __init__(self, start_date: pd.Timestamp, end_date: pd.Timestamp,\n                 trading_start_date: pd.Timestamp=None,\n                 storage_max=10.,\n                 lin_deg_cost=4.,\n                 loss_in=0.95,\n                 loss_out=0.95,\n                 trading_fee=0.09,\n                 num_stor_states=11,\n                 trading_delay=0,\n                 tec_delay=0,\n                 fixed_solve_time=0,\n                 solve_frequency=0.,\n                 withdraw_max=5.,\n                 inject_max=5.,\n                 log_transactions=False,\n                 only_traverse_lob=False,\n                 cycle_limit: float = None,\n                 min_hot_queue_size: int = -1,):\n                #  forecast_horizon_start=10*60,\n                #  forecast_horizon_end=75):\n        \"\"\"\n        Initialize a Simulation instance.\n\n        Args:\n            start_date (pd.Timestamp): The start datetime of the simulation, i.e. which products are loaded into the simulation. Must be timezone aware.\n            end_date (pd.Timestamp): The end datetime of the simulation, i.e. which products are loaded into the simulation. Must be timezone aware.\n            trading_start_date (pd.Timestamp, optional): The start datetime of the trading, i.e. when the trading starts. Must be timezone aware. If None, the trading starts at the same time as the start_date.\n            storage_max (float, optional): The maximum storage capacity of the storage unit (MWh). Default is 10.0.\n            lin_deg_cost (float, optional): The linear degradation cost of the storage unit (\u20ac/MWh). Default is 4.0.\n            loss_in (float, optional): The injection efficiency of the storage unit (0-1]. Default is 0.95.\n            loss_out (float, optional): The withdrawal efficiency of the storage unit (0-1]. Default is 0.95.\n            trading_fee (float, optional): The trading fee for the exchange (\u20ac/MWh). Default is 0.09.\n            num_stor_states (int, optional): The number of storage states for dynamic programming. Default is 11.\n            trading_delay (int, optional): The trading delay of the storage unit, i.e., when to start trading all new products after gate opening. (min, &gt;= 0 and &lt; 480 mins (8 hours)). Default is 0.\n            tec_delay (int, optional): The technical delay of the storage unit (ms, &gt;= 0). Default is 0.\n            fixed_solve_time (int, optional): The fixed solve time for dynamic programming (ms, &gt;= 0 or -1 for realistic solve times). Default is 0.\n            solve_frequency (float, optional): The frequency at which the dynamic programming solver is run (min). Default is 0.0.\n            withdraw_max (float, optional): The maximum withdrawal power of the storage unit (MW). Default is 5.0.\n            inject_max (float, optional): The maximum injection power of the storage unit (MW). Default is 5.0.\n            log_transactions (bool, optional): If True, we run the simulation only to log transactions data of the market, no optimization is performed. Default is False.\n            only_traverse_lob: Whether to only traverse the LOB and not call any DP solves. (bool, default: False)\n            cycle_limit: The limit on the number of cycles per Berlin-time day. Setting it comes at a cost in terms of solve time. (float, &gt; 0). Default is None, where no cycle limit is enforced.\n            min_hot_queue_size: The minimum number of orders to keep in the hot cache for each order queue. (int, &gt; 0, or -1 to disable and use only volume-based caching, default: -1)\n        \"\"\"\n        # forecast_horizon_start (int, optional): The start of the forecast horizon (min). Default is 600.\n        # forecast_horizon_end (int, optional): The end of the forecast horizon (min). Default is 75.\n\n        # write all the assertions\n        if start_date &gt;= end_date:\n            raise ValueError(\"start_date must be before end_date\")\n        if trading_start_date is None:\n            trading_start_date = start_date\n        if trading_start_date &gt;= end_date:\n            raise ValueError(\"trading_start_date must be before end_date\")\n        if storage_max &lt; 0:\n            raise ValueError(\"storage_max must be &gt;= 0\")\n        if lin_deg_cost &lt; 0:\n            raise ValueError(\"lin_deg_cost must be &gt;= 0\")\n        if loss_in &lt; 0 or loss_in &gt; 1:\n            raise ValueError(\"loss_in must be in [0, 1]\")\n        if loss_out &lt; 0 or loss_out &gt; 1:\n            raise ValueError(\"loss_out must be in [0,1]\")\n        if trading_fee &lt; 0:\n            raise ValueError(\"trading_fee must be &gt;= 0\")\n        if num_stor_states &lt;= 0:\n            raise ValueError(\"num_stor_states must be &gt; 0\")\n        if tec_delay &lt; 0:\n            raise ValueError(\"tec_delay must be &gt;= 0\")\n        if fixed_solve_time &lt; 0:\n            if fixed_solve_time != -1:\n                raise ValueError(\"fixed_solve_time must be &gt;= 0 (or -1 for realistic solve times)\")\n        if solve_frequency &lt; 0:\n            raise ValueError(\"solve_frequency must be &gt;= 0\")\n        if withdraw_max &lt;= 0:\n            raise ValueError(\"withdraw_max must be &gt; 0\")\n        if inject_max &lt;= 0:\n            raise ValueError(\"inject_max must be &gt; 0\")\n        if trading_delay &lt; 0 or trading_delay &gt;= 8*60:\n            raise ValueError(\"trading_delay must be &gt;= 0 and &lt; 480 mins (8 hours)\")\n        if cycle_limit is not None:\n            if cycle_limit &lt;= 0:\n                raise ValueError(\"cycle_limit must be &gt; 0 if provided\")\n        if min_hot_queue_size &lt;= 0 and min_hot_queue_size != -1:\n            raise ValueError(\"min_hot_queue_size must be &gt; 0 or -1 (to disable)\")\n        # if forecast_horizon_start &lt; 0:\n        #     raise ValueError(\"forecast_horizon_start must be &gt;= 0\")\n        # if forecast_horizon_end &lt; 0:\n        #     raise ValueError(\"forecast_horizon_end must be &gt;= 0\")\n        # if forecast_horizon_start &lt;= forecast_horizon_end:\n        #     raise ValueError(\"forecast_horizon_start must larger than forecast_horizon_end\")\n\n        self._sim_cpp = Simulation_cpp()\n\n        self._sim_cpp.params.storageMax = storage_max\n        self._sim_cpp.params.linDegCost = lin_deg_cost\n        self._sim_cpp.params.lossIn = loss_in\n        self._sim_cpp.params.lossOut = loss_out\n        self._sim_cpp.params.tradingFee = trading_fee\n        self._sim_cpp.params.numStorStates = num_stor_states\n        self._sim_cpp.params.pingDelay = tec_delay\n        self._sim_cpp.params.fixedSolveTime = fixed_solve_time\n        self._sim_cpp.params.dpFreq = solve_frequency\n        self._sim_cpp.params.withdrawMax = withdraw_max\n        self._sim_cpp.params.injectMax = inject_max\n        self._sim_cpp.params.minuteDelay = trading_delay\n        self._sim_cpp.params.logTransactions = log_transactions\n        self._sim_cpp.params.onlyTraverseLOB = only_traverse_lob\n        if cycle_limit is not None:\n            self._sim_cpp.params.cycleLimit = float(cycle_limit)\n        self._sim_cpp.params.minHotQueueSize = min_hot_queue_size\n        # self._sim_cpp.params.foreHorizonStart = forecast_horizon_start\n        # self._sim_cpp.params.foreHorizonEnd = forecast_horizon_end\n\n        # Set start and end date\n        if start_date &gt;= end_date:\n            raise ValueError(\"start_date must be before end_date\")\n        if start_date.tzinfo is None:\n            raise ValueError(\"start_date must be timezone aware\")\n        start_date = start_date.astimezone(pytz.utc)\n        self._sim_cpp.params.startMonth = start_date.month\n        self._sim_cpp.params.startDay = start_date.day\n        self._sim_cpp.params.startYear = start_date.year\n        self._sim_cpp.params.startHour = start_date.hour\n        self._sim_cpp.params.startMinute = start_date.minute\n        if end_date.tzinfo is None:\n            raise ValueError(\"end_date must be timezone aware\")\n        end_date = end_date.astimezone(pytz.utc)\n        self._sim_cpp.params.endMonth = end_date.month\n        self._sim_cpp.params.endDay = end_date.day\n        self._sim_cpp.params.endYear = end_date.year\n        self._sim_cpp.params.endHour = end_date.hour\n        self._sim_cpp.params.endMinute = end_date.minute\n\n        # Set trading start date\n        if trading_start_date.tzinfo is None:\n            raise ValueError(\"trading_start_date must be timezone aware\")\n        trading_start_date = trading_start_date.astimezone(pytz.utc)\n        self._sim_cpp.params.tradingStartMonth = trading_start_date.month\n        self._sim_cpp.params.tradingStartDay = trading_start_date.day\n        self._sim_cpp.params.tradingStartYear = trading_start_date.year\n        self._sim_cpp.params.tradingStartHour = trading_start_date.hour\n        self._sim_cpp.params.tradingStartMinute = trading_start_date.minute\n\n    def add_bin_to_orderqueue(self, bin_data: str):\n        \"\"\"\n        Add an order binary file to the simulation's order queue.\n\n        Args:\n            bin_data (str): The path to the order binary file.\n        \"\"\"\n        self._sim_cpp.addOrderQueueFromBin(bin_data)\n\n    def add_df_to_orderqueue(self, df: pd.DataFrame):\n        \"\"\"\n        Add a DataFrame of orders to the simulation's order queue.\n\n        The DataFrame must have the same columns as the saved CSV files, with timestamps in UTC\n        (seconds and milliseconds).\n\n        Args:\n            df (pd.DataFrame): A DataFrame containing the orders to be added.\n\n        Processing Steps:\n            - Validate that the timestamp columns ('start', 'transaction', 'validity') are timezone aware.\n            - Ensure that all timestamps are in the same timezone.\n            - Convert all timestamps to UTC and format them in ISO 8601.\n        \"\"\"\n        if (df[\"start\"].dt.tz is None and df[\"transaction\"].dt.tz is None and df[\"validity\"].dt.tz is None):\n            raise ValueError(\"All timestamps of input df must be timezone aware\")\n        if not (df[\"start\"].dt.tz == df[\"transaction\"].dt.tz and df[\"start\"].dt.tz == df[\"validity\"].dt.tz):\n            raise ValueError(\"All timestamps of input df must be in the same timezone\")\n\n        df[\"start\"] = df[\"start\"].dt.tz_convert(\"UTC\")\n        df[\"transaction\"] = df[\"transaction\"].dt.tz_convert(\"UTC\")\n        df[\"validity\"] = df[\"validity\"].dt.tz_convert(\"UTC\")\n        df[\"start\"] = df[\"start\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n        df[\"transaction\"] = df[\"transaction\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n        df[\"validity\"] = df[\"validity\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n\n        ids = df['id'].to_numpy(dtype=np.int64).tolist()\n        initials = df['initial'].to_numpy(dtype=np.int64).tolist()\n        sides = df['side'].to_numpy(dtype='str').tolist()\n        starts = df['start'].to_numpy(dtype='str').tolist()\n        transactions = df['transaction'].to_numpy(dtype='str').tolist()\n        validities = df['validity'].to_numpy(dtype='str').tolist()\n        prices = df['price'].to_numpy(dtype=np.float64).tolist()\n        quantities = df['quantity'].to_numpy(dtype=np.float64).tolist()\n\n        self._sim_cpp.addOrderQueueFromPandas(ids, initials, sides, starts, transactions, validities, prices, quantities)\n\n    # def add_forecast_from_df(self, df: pd.DataFrame):\n    #     \"\"\"\n    #     Add forecast data from a DataFrame to the simulation.\n\n    #     The DataFrame must contain the following columns:\n    #         - creation_time: The time when the forecast was created (timezone aware, up to millisecond precision).\n    #         - delivery_start: The start time of the delivery period (timezone aware).\n    #         - sell_price: The price at which the optimization will try to sell (\u20ac/MWh).\n    #         - buy_price: The price at which the optimization will try to buy (\u20ac/MWh).\n\n    #     Args:\n    #         df (pd.DataFrame): A DataFrame containing the forecast data.\n\n    #     Processing Steps:\n    #         - Validate that the 'creation_time' and 'delivery_start' columns are timezone aware and identical.\n    #         - Convert the timestamps to UTC and format them in ISO 8601.\n    #         - Pass the data to the simulation.\n    #     \"\"\"\n    #     if (df[\"creation_time\"].dt.tz is None and df[\"delivery_start\"].dt.tz is None):\n    #         raise ValueError(\"All timestamps of input df must be timezone aware\")\n    #     if not (df[\"creation_time\"].dt.tz == df[\"delivery_start\"].dt.tz):\n    #         raise ValueError(\"All timestamps of input df must be in the same timezone\")\n\n    #     df[\"creation_time\"] = df[\"creation_time\"].dt.tz_convert(\"UTC\")\n    #     df[\"delivery_start\"] = df[\"delivery_start\"].dt.tz_convert(\"UTC\")\n\n    #     df[\"creation_time\"] = df[\"creation_time\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n    #     df[\"delivery_start\"] = df[\"delivery_start\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n\n    #     creation_time = df[\"creation_time\"].to_numpy(dtype='str').tolist()\n    #     delivery_start = df[\"delivery_start\"].to_numpy(dtype='str').tolist()\n    #     buy_price = df[\"buy_price\"].to_numpy(dtype=np.float64).tolist()\n    #     sell_price = df[\"sell_price\"].to_numpy(dtype=np.float64).tolist()\n\n    #     self._sim_cpp.loadForecastMapFromPandas(creation_time, delivery_start, buy_price, sell_price)\n\n    def get_data_bins_for_each_day(self, base_path: str, start_date: pd.Timestamp, end_date: pd.Timestamp):\n        \"\"\"\n        Generate a list of file paths for binary order book data for each day within a date range.\n\n        Args:\n            base_path (str): The base directory path where the binary files are stored.\n            start_date (pd.Timestamp): The start date of the range.\n            end_date (pd.Timestamp): The end date of the range.\n\n        Returns:\n            list: A list of file paths for each day's binary order book file.\n        \"\"\"\n        # convert dates to utc time\n        start_date_berlin = start_date.tz_convert('Europe/Berlin') # convert to tz in which the lob files are segemented\n        end_date_berlin = end_date.tz_convert('Europe/Berlin') # convert to tz in which the lob files are segemented\n\n        # round up to midnight\n        end_date_berlin_round_up = end_date_berlin.replace(hour=23, minute=59, second=59)\n\n        base_path = os.path.join(base_path, '')\n        base_path += \"orderbook_\"\n\n        # Generate paths for each day within the date range\n        paths = []\n\n        current_date = start_date_berlin - timedelta(days=1) # include the day before the start date to ensure that all orders submitted with delivery on first day are included\n        while current_date &lt; end_date_berlin_round_up:\n            path = f\"{base_path}{current_date.strftime('%Y-%m-%d')}.bin\"\n            paths.append(path)\n            current_date += timedelta(days=1)\n\n        return paths\n\n    def run(self, data_path: str, verbose: bool = True):\n        \"\"\"\n        Execute the simulation using binary data files.\n\n        The files must be named as: orderbook_YYYY-MM-DD.bin.\n\n        Args:\n            data_path (str): The directory containing the binary data files.\n            verbose (bool, optional): If True, display progress logs. Default is True.\n\n        Processing Steps:\n            - Retrieve the list of binary file paths for the simulation period.\n            - Iterate through each day's data, add the file to the order queue, and run the simulation for that day.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the transactions if log_transactions is True, otherwise None.\n\n        \"\"\"\n        start_date = pd.Timestamp(year=self._sim_cpp.params.startYear,\n                                  month=self._sim_cpp.params.startMonth,\n                                  day=self._sim_cpp.params.startDay,\n                                  hour=self._sim_cpp.params.startHour,\n                                  minute=self._sim_cpp.params.startMinute,\n                                  tz=\"UTC\")\n        end_date = pd.Timestamp(year=self._sim_cpp.params.endYear,\n                                month=self._sim_cpp.params.endMonth,\n                                day=self._sim_cpp.params.endDay,\n                                hour=self._sim_cpp.params.endHour,\n                                minute=self._sim_cpp.params.endMinute,\n                                tz=\"UTC\")\n        lob_paths = self.get_data_bins_for_each_day(data_path, start_date, end_date)\n\n        transactions = pd.DataFrame()\n\n        num_days = len(lob_paths)\n        if verbose: print(\"The simulation will iterate over\", num_days, \"files.\")\n\n        with tqdm(total=num_days, desc=\"Simulated Days\", unit=\"%\", ncols=120, disable=not verbose) as pbar:\n            for i, path in enumerate(lob_paths):\n                pbar.set_description(f\"Currently simulating {path.split('/')[-1]} ... \")\n                self.add_bin_to_orderqueue(path)\n                self.run_one_day(i == len(lob_paths) - 1)\n                if self._sim_cpp.params.logTransactions:\n                    transactions = pd.concat([transactions, self.group_transactions(self.get_transactions())])\n                pbar.update(1)\n\n        if verbose: print(\"Simulation finished.\")\n\n        if self._sim_cpp.params.logTransactions and not transactions.empty:\n            return transactions\n\n    def group_transactions(self, transactions: pd.DataFrame):\n        \"\"\"\n        Group transactions by timestamp and delivery hour, calculating volume-weighted average prices.\n\n        Args:\n            transactions (pd.DataFrame): A DataFrame containing the transactions to be grouped.\n\n        Processing Steps:\n            - Group the transactions by timestamp and delivery_hour.\n            - Calculate the volume weighted average price for each group.\n            - Return a DataFrame with aggregated transaction data.\n\n        Returns:\n            pd.DataFrame: A DataFrame with the following columns:\n                - timestamp: The UTC timestamp when the transaction occurred.\n                - delivery_hour: The UTC timestamp of the delivery hour for the traded product.\n                - vwap: The volume weighted average price of the transaction.\n                - total_volume: The total volume of the transaction.\n                - num_transactions: The number of transactions in the group.\n        \"\"\"\n\n        vwap_results = []\n\n        # Group by timestamp and delivery_hour\n        grouped = transactions.groupby(['timestamp', 'delivery_hour'])\n\n        for (timestamp, delivery_hour), group in grouped:\n            if len(group) == 1:\n                # Single transaction - use price and volume directly\n                row = group.iloc[0]\n                vwap = row['price']\n                total_volume = row['volume']\n            else:\n                # Multiple transactions - calculate volume weighted average price\n                total_volume = group['volume'].sum()\n                weighted_price_sum = (group['price'] * group['volume']).sum()\n                vwap = weighted_price_sum / total_volume if total_volume &gt; 0 else 0\n\n            vwap_results.append({\n                'timestamp': timestamp,\n                'delivery_hour': delivery_hour,\n                'vwap': vwap,\n                'total_volume': total_volume,\n                'num_transactions': len(group)\n            })\n\n        if vwap_results:\n            vwap_df = pd.DataFrame(vwap_results)\n        else:\n            vwap_df = pd.DataFrame()\n\n        return vwap_df\n\n    def run_one_day(self, is_last: bool):\n        \"\"\"\n        Run the simulation for a single day.\n\n        Args:\n            is_last (bool): If True, indicates that this is the last iteration of data.\n\n        Processing Steps:\n            - Execute the simulation for the provided day's data.\n        \"\"\"\n        self._sim_cpp.run(is_last)\n\n    def get_logs(self):\n        \"\"\"\n        Retrieve the logs generated by the simulation.\n\n        Returns:\n            dict: A dictionary containing simulation logs with the following keys:\n                - decision_record: Final simulation schedule.\n                - price_record: CID price data over the simulation duration.\n                - accepted_orders: Limit orders accepted by the RI.\n                - executed_orders: Orders sent to the exchange by the RI.\n                - killed_orders: Orders that were missed at the exchange.\n        \"\"\"\n        # - forecast_orders: Orders virtually traded against the forecast.\n        # - balancing_orders: Orders that would have incurred payments to the TSO.\n        decision_record, price_record, accepted_orders, executed_orders, forecast_orders, killed_orders, balancing_orders = self._sim_cpp.getLogs()\n        decision_record = pd.DataFrame(decision_record)\n        price_record = pd.DataFrame(price_record)\n        accepted_orders = pd.DataFrame(accepted_orders)\n        executed_orders = pd.DataFrame(executed_orders)\n        forecast_orders = pd.DataFrame(forecast_orders)\n        killed_orders = pd.DataFrame(killed_orders)\n        balancing_orders = pd.DataFrame(balancing_orders)\n\n        if not decision_record.empty:\n            decision_record[\"hour\"] = pd.to_datetime(decision_record[\"hour\"], utc=True)\n            decision_record[\"cycles\"] = np.round(decision_record[\"cycles\"].astype(float), 2)\n        if not price_record.empty:\n            price_record[\"hour\"] = pd.to_datetime(price_record[\"hour\"], utc=True)\n        if not accepted_orders.empty:\n            accepted_orders[\"time\"] = pd.to_datetime(accepted_orders[\"time\"], utc=True)\n            accepted_orders[\"start\"] = pd.to_datetime(accepted_orders[\"start\"], utc=True)\n            accepted_orders[\"cancel\"] = pd.to_datetime(accepted_orders[\"cancel\"], utc=True)\n            accepted_orders[\"delivery\"] = pd.to_datetime(accepted_orders[\"delivery\"], utc=True)\n        if not executed_orders.empty:\n            executed_orders[\"time\"] = pd.to_datetime(executed_orders[\"time\"], utc=True)\n            executed_orders[\"last_solve_time\"] = pd.to_datetime(executed_orders[\"last_solve_time\"], utc=True)\n            executed_orders[\"hour\"] = pd.to_datetime(executed_orders[\"hour\"], utc=True)\n        if not forecast_orders.empty:\n            forecast_orders[\"time\"] = pd.to_datetime(forecast_orders[\"time\"], utc=True)\n            forecast_orders[\"last_solve_time\"] = pd.to_datetime(forecast_orders[\"last_solve_time\"], utc=True)\n            forecast_orders[\"hour\"] = pd.to_datetime(forecast_orders[\"hour\"], utc=True)\n        if not killed_orders.empty:\n            killed_orders[\"time\"] = pd.to_datetime(killed_orders[\"time\"], utc=True)\n            killed_orders[\"last_solve_time\"] = pd.to_datetime(killed_orders[\"last_solve_time\"], utc=True)\n            killed_orders[\"hour\"] = pd.to_datetime(killed_orders[\"hour\"], utc=True)\n        if not balancing_orders.empty:\n            balancing_orders[\"time\"] = pd.to_datetime(balancing_orders[\"time\"], utc=True)\n            balancing_orders[\"hour\"] = pd.to_datetime(balancing_orders[\"hour\"], utc=True)\n\n        logs = {\n            \"decision_record\": pd.DataFrame(decision_record, index=None),\n            \"price_record\": pd.DataFrame(price_record, index=None),\n            \"accepted_orders\": pd.DataFrame(accepted_orders, index=None),\n            \"executed_orders\": pd.DataFrame(executed_orders, index=None),\n            # \"forecast_orders\": pd.DataFrame(forecast_orders, index=None), # removed for later versions of the code\n            \"killed_orders\": pd.DataFrame(killed_orders, index=None),\n            # \"balancing_orders\": pd.DataFrame(balancing_orders, index=None), # removed for later versions of the code\n        }\n        return logs\n\n    def get_transactions(self):\n        \"\"\"\n        Retrieve all transactions that have occurred since the last call and clear the internal transaction log.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing all transactions that occurred, with the following columns:\n                - timestamp: The UTC timestamp when the transaction occurred.\n                - delivery_hour: The UTC timestamp of the delivery hour for the traded product.\n                - price: The execution price of the transaction (EUR/MWh).\n                - volume: The volume of the transaction (MW).\n                - buy_order_type: The type of the buy order ('Market' or 'Limit').\n                - sell_order_type: The type of the sell order ('Market' or 'Limit').\n                - buy_order_id: The ID of the buy order.\n                - sell_order_id: The ID of the sell order.\n        \"\"\"\n        transactions = self._sim_cpp.getTransactions()\n        transactions = pd.DataFrame(transactions)\n        if not transactions.empty:\n            transactions[\"timestamp\"] = pd.to_datetime(transactions[\"timestamp\"], utc=True)\n            transactions[\"delivery_hour\"] = pd.to_datetime(transactions[\"delivery_hour\"], utc=True)\n        return transactions\n\n    def print_parameters(self):\n        \"\"\"\n        Print the simulation parameters, including start/end times, storage settings, and various limits and costs.\n\n        Processing Steps:\n            - Extract simulation start, end, and trading start times from internal parameters.\n            - Display all relevant storage configuration parameters.\n            - Show trading and technical constraints.\n        \"\"\"\n        startMonth = self._sim_cpp.params.startMonth\n        startDay = self._sim_cpp.params.startDay\n        startYear = self._sim_cpp.params.startYear\n        startHour = self._sim_cpp.params.startHour\n        startMinute = self._sim_cpp.params.startMinute\n        endMonth = self._sim_cpp.params.endMonth\n        endDay = self._sim_cpp.params.endDay\n        endYear = self._sim_cpp.params.endYear\n        endHour = self._sim_cpp.params.endHour\n        endMinute = self._sim_cpp.params.endMinute\n        tradingStartMonth = self._sim_cpp.params.tradingStartMonth\n        tradingStartDay = self._sim_cpp.params.tradingStartDay\n        tradingStartYear = self._sim_cpp.params.tradingStartYear\n        tradingStartHour = self._sim_cpp.params.tradingStartHour\n        tradingStartMinute = self._sim_cpp.params.tradingStartMinute\n        cycleLimit = self._sim_cpp.params.cycleLimit\n\n        startDate = pd.Timestamp(year=startYear, month=startMonth, day=startDay, hour=startHour, minute=startMinute, tz=\"UTC\")\n        endDate = pd.Timestamp(year=endYear, month=endMonth, day=endDay, hour=endHour, minute=endMinute, tz=\"UTC\")\n        tradingStartDate = pd.Timestamp(year=tradingStartYear, month=tradingStartMonth, day=tradingStartDay, hour=tradingStartHour, minute=tradingStartMinute, tz=\"UTC\")\n\n        print(\"Start Time (UTC):\", startDate)\n        print(\"End Time (UTC):\", endDate)\n        print(\"Trading Start Time (UTC):\", tradingStartDate)\n\n        print(\"Storage Maximum:\", self._sim_cpp.params.storageMax, \"MWh\")\n        print(\"Linear Degredation Cost:\", self._sim_cpp.params.linDegCost, \"\u20ac/MWh\")\n        print(\"Injection Loss \u03b7+:\", self._sim_cpp.params.lossIn)\n        print(\"Withdrawal Loss \u03b7-:\", self._sim_cpp.params.lossOut)\n        print(\"Trading Fee:\", self._sim_cpp.params.tradingFee, \"\u20ac/MWh\")\n        print(\"Number of DP Storage States:\", self._sim_cpp.params.numStorStates)\n        print(\"Technical Delay:\", self._sim_cpp.params.pingDelay, \"ms\")\n        print(\"Trading Delay:\", self._sim_cpp.params.minuteDelay, \"min\")\n        print(\"Fixed Solve Time:\", self._sim_cpp.params.fixedSolveTime, \"ms\")\n        print(\"Solve Frequency:\", self._sim_cpp.params.dpFreq, \"min\")\n        print(\"Injection Maximum:\", self._sim_cpp.params.injectMax, \"MW\")\n        print(\"Withdrawal Maximum:\", self._sim_cpp.params.withdrawMax, \"MW\")\n        print(\"Log Transactions:\", self._sim_cpp.params.logTransactions)\n        print(\"Only Traverse LOB:\", self._sim_cpp.params.onlyTraverseLOB)\n        print(\"Cycle Limit:\", cycleLimit)\n        # print(\"Forecast Horizon Start:\", self._sim_cpp.params.foreHorizonStart, \"min\")\n        # print(\"Forecast Horizon End:\", self._sim_cpp.params.foreHorizonEnd, \"min\")\n\n    def return_vol_price_pairs(self, is_last: bool, frequency: int, volumes: np.ndarray):\n        \"\"\"\n        Retrieve volume-price pairs from the simulation.\n\n        Args:\n            is_last (bool): If True, indicates this is the last iteration of data.\n            frequency (int): The frequency (in seconds) at which price data is retrieved.\n            volumes (np.ndarray): A 1D numpy array of volumes for which prices are returned.\n\n        Processing Steps:\n            - Validate input parameters for correct format and values.\n            - Extract volume-price data from the simulation at specified frequency.\n            - Convert timestamps to UTC format for consistency.\n\n        Returns:\n            pd.DataFrame: A DataFrame with columns:\n                - current_time: Time of the export (UTC).\n                - delivery_hour: Delivery period time (UTC).\n                - volume: The volume for which the price is exported (MWh).\n                - price_full: The full price (cashflow) for the volume (\u20ac).\n                - worst_accepted_price: Market price of the worst matched order (\u20ac/MWh).\n        \"\"\"\n        if len(volumes.shape) != 1:\n            raise ValueError(\"volumes must be a 1D numpy array\")\n        if frequency &lt;= 0:\n            raise ValueError(\"frequency must be &gt; 0\")\n\n        vol_price_list = self._sim_cpp.return_vol_price_pairs(is_last, frequency, volumes)\n        vol_price_list = pd.DataFrame(vol_price_list)\n\n        if not vol_price_list.empty:\n            vol_price_list[\"current_time\"] = pd.to_datetime(vol_price_list[\"current_time\"], utc=True)\n            vol_price_list[\"delivery_hour\"] = pd.to_datetime(vol_price_list[\"delivery_hour\"], utc=True)\n\n        return vol_price_list\n\n    def submit_limit_orders(self, df: pd.DataFrame):\n        \"\"\"\n        Submit a list of limit orders and track their matches without battery optimization.\n\n        This method validates input data and queues the limit orders for submission at specified times.\n        The orders will be submitted during the normal simulation run without triggering battery optimization.\n\n        Args:\n            df (pd.DataFrame): A DataFrame containing the limit orders to be submitted.\n                The DataFrame must have the following columns:\n                    - transaction_time: The time when the order should be submitted (timezone aware, up to millisecond precision).\n                    - price: The price of the limit order (\u20ac/MWh).\n                    - volume: The volume of the limit order (MWh, positive for buy, negative for sell).\n                    - side: The side of the order ('buy' or 'sell').\n                    - delivery_time: The delivery time for the order (timezone aware, required).\n\n        Processing Steps:\n            - Validates input data format and required columns.\n            - Ensures timezone awareness and proper formatting of timestamps.\n            - Queues the limit orders for submission during simulation execution.\n            - Orders are processed without triggering battery optimization.\n\n        Returns:\n            None: This method queues orders but does not return match information.\n                After running the simulation, use get_limit_order_matches() to retrieve match details.\n\n        Note:\n            Call this method to queue own limit orders, then run the simulation to process them and collect matches.\n            Use get_limit_order_matches() after simulation to retrieve final match results.\n        \"\"\"\n\n        # Validate input DataFrame\n        required_columns = ['transaction_time', 'price', 'volume', 'side', 'delivery_time']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            raise ValueError(f\"Missing required columns: {missing_columns}\")\n\n        # Check if transaction_time is timezone aware\n        if df[\"transaction_time\"].dt.tz is None:\n            raise ValueError(\"transaction_time must be timezone aware\")\n        if df[\"transaction_time\"].isna().any():\n            raise ValueError(\"transaction_time cannot contain NaT values - all transaction times are required\")\n\n        # Check if delivery_time is timezone aware and has no NaT values\n        if df[\"delivery_time\"].dt.tz is None:\n            raise ValueError(\"delivery_time must be timezone aware\")\n        if df[\"delivery_time\"].isna().any():\n            raise ValueError(\"delivery_time cannot contain NaT values - all delivery times are required\")\n\n        # check that the delivery time is a full hour exactly\n        if (df[\"delivery_time\"].dt.minute != 0).any():\n            raise ValueError(\"delivery_time must be a full hour exactly for hourly products\")\n\n        # volume must be &gt; 0\n        if df[\"volume\"].le(0).any():\n            raise ValueError(\"volume must be &gt; 0\")\n\n        # Convert to UTC\n        df = df.copy()\n        df[\"transaction_time\"] = df[\"transaction_time\"].dt.tz_convert(\"UTC\")\n        df[\"delivery_time\"] = df[\"delivery_time\"].dt.tz_convert(\"UTC\")\n\n        # Validate side column\n        valid_sides = {'buy', 'sell', 'Buy', 'Sell', 'BUY', 'SELL'}\n        invalid_sides = df[\"side\"].unique()\n        invalid_sides = [side for side in invalid_sides if side not in valid_sides]\n        if invalid_sides:\n            raise ValueError(f\"Invalid side values: {invalid_sides}. Must be one of: {valid_sides}\")\n\n        # Normalize side values to 'Buy'/'Sell'\n        df[\"side\"] = df[\"side\"].str.capitalize()\n\n        # Convert timestamps to ISO format\n        df[\"transaction_time\"] = df[\"transaction_time\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n        df[\"delivery_time\"] = df[\"delivery_time\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n\n        # Prepare data for C++ function\n        transaction_times = df[\"transaction_time\"].to_numpy(dtype='str').tolist()\n        prices = df[\"price\"].to_numpy(dtype=np.float64).tolist()\n        volumes = df[\"volume\"].to_numpy(dtype=np.float64).tolist()\n        sides = df[\"side\"].to_numpy(dtype='str').tolist()\n        delivery_times = df[\"delivery_time\"].to_numpy(dtype='str').tolist()\n\n        # Call C++ function to submit limit orders (no return value)\n        self._sim_cpp.submitLimitOrdersAndGetMatches(transaction_times, prices, volumes, sides, delivery_times)\n\n    def get_limit_order_matches(self):\n        \"\"\"\n        Get the limit order matches collected during simulation using the forecast tracking mechanism.\n\n        This method retrieves match information for submitted limit orders that were processed\n        during the simulation run. The orders are tracked using the forecast flag system.\n\n        Processing Steps:\n            - Retrieves match data from the C++ simulation backend.\n            - Converts timestamps to timezone-aware datetime objects.\n            - Clears the internal match storage after retrieval.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing information about which limit orders were matched against which existing orders.\n                The DataFrame contains the following columns:\n                    - submitted_order_id: The ID of the submitted limit order.\n                    - matched_order_id: The ID of the existing order that was matched.\n                    - match_timestamp: The timestamp when the match occurred.\n                    - delivery_hour: The delivery hour for the matched order.\n                    - match_price: The price at which the orders were matched, i.e., the price of the existing (partially) matched order (\u20ac/MWh).\n                    - match_volume: The volume that was matched (MWh).\n                    - submitted_order_side: The side of the submitted order ('buy' or 'sell').\n                    - existing_order_side: The side of the existing order ('buy' or 'sell').\n        \"\"\"\n        matches = self._sim_cpp.getLimitOrderMatches()\n        matches_df = pd.DataFrame(matches)\n\n        # Convert timestamps to datetime if not empty\n        if not matches_df.empty:\n            matches_df[\"match_timestamp\"] = pd.to_datetime(matches_df[\"match_timestamp\"], utc=True)\n            matches_df[\"delivery_hour\"] = pd.to_datetime(matches_df[\"delivery_hour\"], utc=True)\n        else:\n            print(\"No limit order matches to return.\")\n\n        self._sim_cpp.clearLimitOrderMatches() # clear existing matches\n\n        return matches_df\n\n    def has_orders_remaining(self) -&gt; bool:\n        \"\"\"\n        Check if there are remaining orders in the order queue.\n\n        Returns\n        -------\n        bool\n            True if there are remaining orders in the queue, False otherwise.\n        \"\"\"\n        return self._sim_cpp.hasOrdersRemaining()\n\n    def set_stop_time(self, stop_time: pd.Timestamp, verbose: bool = False):\n        \"\"\"\n        Set a datetime with millisecond precision to stop the simulation once.\n\n        The simulation will stop only once, if the last order added has a submission time\n        after the stop time. Once the simulation has stopped, the stop time is automatically\n        cleared, allowing you to set a new one.\n\n        Parameters\n        ----------\n        stop_time : pd.Timestamp\n            A timezone-aware timestamp with millisecond precision when the simulation should stop.\n            The simulation will stop if the last processed order's submission time is &gt; this stop time.\n        verbose : bool, optional\n            Whether to print a message when the simulation stops.\n        Raises\n        ------\n        ValueError\n            If stop_time is not timezone aware.\n\n        Notes\n        -----\n        - The stop time is checked after each order is processed\n        - The simulation stops only once per stop time setting\n        - After stopping, the stop time is automatically cleared\n        - You can set a new stop time after the simulation has stopped\n        - The stop time is compared against the order's submission time (transaction time)\n\n        \"\"\"\n        # Check if timezone aware\n        if stop_time.tzinfo is None:\n            raise ValueError(\"stop_time must be timezone aware\")\n\n        # Convert to UTC\n        stop_time_utc = stop_time.astimezone(pytz.utc)\n\n        # Convert to milliseconds since epoch\n        stop_time_ms = int(stop_time_utc.timestamp() * 1000)\n\n        # Call C++ method\n        self._sim_cpp.setStopTime(stop_time_ms, verbose)\n\n    def solve(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Solve the dynamic programming problem once using the time of the last placed order.\n\n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame containing the suggested to be executed market orders from the solve.\n            Columns include: dp_run, time, last_solve_time, hour, reward,\n            reward_incl_deg_costs, volume, type, final_pos, final_stor.\n\n        Notes\n        -----\n        This function calls the C++ solve() method once. It does not run the\n        full simulation, only performs a single DP solve at the time of the\n        last placed order. If no orders have been placed yet, the behavior\n        depends on the initial state of _lastOrder_placementTime.\n\n        Example\n        -------\n        &gt;&gt;&gt; orders_df = sim.solve()\n        \"\"\"\n        # Call C++ method (no parameters needed - uses last order placement time)\n        order_list = self._sim_cpp.solve()\n\n        # Convert list of dicts to pandas DataFrame\n        if not order_list:\n            # Return empty DataFrame with expected columns\n            return pd.DataFrame(columns=[\n                'dp_run', 'time', 'last_solve_time', 'hour', 'reward',\n                'reward_incl_deg_costs', 'volume', 'type', 'final_pos', 'final_stor'\n            ])\n\n        order_df = pd.DataFrame(order_list)\n        order_df['time'] = pd.to_datetime(order_df['time'], utc=True)\n        order_df['last_solve_time'] = pd.to_datetime(order_df['last_solve_time'], utc=True)\n        order_df['hour'] = pd.to_datetime(order_df['hour'], utc=True)\n\n        return order_df\n\n    def transform_lob_to_levels(\n        self,\n        lob_state_df: pd.DataFrame,\n        exchange: str = \"EPEX\",\n        product_name: str = \"XBID_Hour_Power\",\n        delivery_area: str = \"10YDE-VE-------2\",\n        product_duration_hours: int = 1\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Transforms the output of get_limit_order_book_state (individual orders)\n        into an aggregated, price-level-based DataFrame.\n\n        Parameters\n        ----------\n        lob_state_df : pd.DataFrame\n            The DataFrame returned by get_limit_order_book_state.\n            Must contain columns: 'delivery_time', 'side', 'price', 'volume'.\n        exchange : str, optional\n            Static value for the 'exchange' column in the output.\n        product_name : str, optional\n            Static value for the 'product' column in the output.\n        delivery_area : str, optional\n            Static value for the 'deliveryArea' column in the output.\n        product_duration_hours : int, optional\n            Duration of the product in hours, used to calculate\n            deliveryEndUtc from delivery_time (which is used as deliveryStartUtc).\n            Default is 1.\n\n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame in the target format with aggregated price levels and columns:\n            ['exchange', 'product', 'deliveryStartUtc', 'deliveryEndUtc',\n            'deliveryArea', 'side', 'level', 'price', 'quantity']\n        \"\"\"\n\n        # Define the target column order\n        target_columns = [\n            'exchange', 'product', 'deliveryStartUtc', 'deliveryEndUtc',\n            'deliveryArea', 'side', 'level', 'price', 'quantity'\n        ]\n\n        # Keep only the columns we need from the start\n        source_columns = ['delivery_time', 'side', 'price', 'volume']\n\n        # Handle empty input DataFrame\n        if lob_state_df.empty:\n            return pd.DataFrame(columns=target_columns)\n\n        all_orders_dfs = []\n        product_duration = pd.Timedelta(hours=product_duration_hours)\n\n        # Group by each product (identified by its delivery_time)\n        for delivery_time, product_df in lob_state_df[source_columns].groupby('delivery_time'):\n\n            delivery_start_utc = delivery_time\n            delivery_end_utc = delivery_start_utc + product_duration\n\n            # --- Process Bids (Source 'buy' side) ---\n            buys_df = product_df[product_df['side'] == 'buy']\n            if not buys_df.empty:\n                # We assume buys_df is already sorted by price (descending)\n                bids_1_to_1 = buys_df.copy().reset_index(drop=True)\n\n                # Set level as the order rank\n                bids_1_to_1['level'] = bids_1_to_1.index\n                bids_1_to_1['side'] = 'bid'\n                bids_1_to_1 = bids_1_to_1.rename(columns={'volume': 'quantity'})\n\n                # Add common metadata\n                bids_1_to_1['exchange'] = exchange\n                bids_1_to_1['product'] = product_name\n                bids_1_to_1['deliveryStartUtc'] = delivery_start_utc\n                bids_1_to_1['deliveryEndUtc'] = delivery_end_utc\n                bids_1_to_1['deliveryArea'] = delivery_area\n\n                all_orders_dfs.append(bids_1_to_1[target_columns]) # Select only target cols\n\n            # --- Process Asks (Source 'sell' side) ---\n            sells_df = product_df[product_df['side'] == 'sell']\n            if not sells_df.empty:\n                # We assume sells_df is already sorted by price (ascending)\n                asks_1_to_1 = sells_df.copy().reset_index(drop=True)\n\n                # Set level as the order rank\n                asks_1_to_1['level'] = asks_1_to_1.index\n                asks_1_to_1['side'] = 'ask'\n                asks_1_to_1 = asks_1_to_1.rename(columns={'volume': 'quantity'})\n\n                # Add common metadata\n                asks_1_to_1['exchange'] = exchange\n                asks_1_to_1['product'] = product_name\n                asks_1_to_1['deliveryStartUtc'] = delivery_start_utc\n                asks_1_to_1['deliveryEndUtc'] = delivery_end_utc\n                asks_1_to_1['deliveryArea'] = delivery_area\n\n                all_orders_dfs.append(asks_1_to_1[target_columns]) # Select only target cols\n\n        # Handle case where input was not empty but contained no orders\n        if not all_orders_dfs:\n            return pd.DataFrame(columns=target_columns)\n\n        # Combine all products and sides into one DataFrame\n        final_df = pd.concat(all_orders_dfs, ignore_index=True)\n\n        return final_df\n\n    def get_limit_order_book_state(self, max_action: float = None, return_dict: bool = False):\n        \"\"\"\n        Get the current state of all active limit order books at the last-set stop time.\n\n        This method returns, for each tradable product (delivery hour), the individual limit orders\n        in the buy and sell queues with all their attributes, up to a cumulative volume of max_action.\n        The query time is automatically set to the last-set stop time.\n\n        Parameters\n        ----------\n        max_action : float, optional\n            The maximum cumulative volume to query in MW (= MWh for 1-hour products).\n            If None (default), uses inject_max + withdraw_max from simulation parameters.\n            Must be &gt; 0 if specified.\n        return_dict : bool, optional\n            Whether to return the limit order book state as a dictionary.\n            If True, the return value is a dictionary with the delivery time as the key and the limit order book state as the value.\n            If False (default), the return value is a DataFrame.\n\n        Returns\n        -------\n        pd.DataFrame or dict\n            A DataFrame containing the limit orders with the following columns:\n                - delivery_time: The delivery time of the product (UTC timestamp)\n                - side: 'sell' or 'buy' (sell orders are where you can buy from, buy orders are where you can sell to)\n                - order_id: The unique order ID\n                - initial_id: The initial order ID (for tracking order modifications)\n                - start_time: When the order was placed (UTC timestamp)\n                - cancel_time: When the order expires (UTC timestamp)\n                - price: The limit order price in EUR/MWh\n                - volume: The order volume in MWh\n                - is_forecast: Whether this is a forecast order (bool)\n                - cumulative_volume: The cumulative volume up to and including this order in MWh\n\n        Notes\n        -----\n        - Uses the simulation's internal current time (_lastOrder_placementTime)\n        - Sell orders are sorted by ascending price (cheapest first = best for buying)\n        - Buy orders are sorted by descending price (highest first = best for selling)\n        - Orders are filtered to exclude expired orders at the query time\n        - Cumulative volume stops at max_action (default: inject_max + withdraw_max)\n        - Each row represents one limit order in the order book\n\n        Example\n        -------\n        &gt;&gt;&gt; # Query order book state with default max_action (inject_max + withdraw_max)\n        &gt;&gt;&gt; lob_state = sim.get_limit_order_book_state()\n        &gt;&gt;&gt; # Query with custom max_action\n        &gt;&gt;&gt; lob_state = sim.get_limit_order_book_state(max_action=20.0)\n        &gt;&gt;&gt; # Filter to see sell orders for a specific product\n        &gt;&gt;&gt; product_sells = lob_state[(lob_state['delivery_time'] == some_time) &amp; (lob_state['side'] == 'sell')]\n        &gt;&gt;&gt; # See the best (cheapest) sell price\n        &gt;&gt;&gt; best_sell_price = product_sells.iloc[0]['price']\n        &gt;&gt;&gt; # Filter to see only forecast orders\n        &gt;&gt;&gt; forecast_orders = lob_state[lob_state['is_forecast'] == True]\n        \"\"\"\n        # Determine max_action value\n        if max_action is None:\n            # Use inject_max + withdraw_max as default\n            max_action_value = self._sim_cpp.params.injectMax + self._sim_cpp.params.withdrawMax\n        else:\n            # Validate that max_action is positive\n            if max_action &lt;= 0:\n                raise ValueError(\"max_action must be &gt; 0\")\n            max_action_value = max_action\n\n        # Call C++ function (uses simulation's current time internally)\n        lob_state_dict = self._sim_cpp.getLimitOrderBookState(max_action_value)\n\n        if return_dict:\n            return lob_state_dict\n\n        # Convert to DataFrame\n        rows = []\n        for delivery_time_ms, data in lob_state_dict.items():\n            delivery_time = pd.Timestamp(delivery_time_ms, unit='ms', tz='UTC')\n\n            # Process sell orders (where we can buy from)\n            sell_ids = data['sell_ids']\n            sell_initial_ids = data['sell_initial_ids']\n            sell_starts = data['sell_starts']\n            sell_cancels = data['sell_cancels']\n            sell_prices = data['sell_prices']\n            sell_volumes = data['sell_volumes']\n            sell_forecasts = data['sell_forecasts']\n\n            cumulative_sell_volume = 0.0\n            for order_id, initial_id, start, cancel, price, volume, forecast in zip(\n                sell_ids, sell_initial_ids, sell_starts, sell_cancels, sell_prices, sell_volumes, sell_forecasts\n            ):\n                cumulative_sell_volume += volume\n                # Handle sentinel cancel time values (e.g., max int64) that overflow pandas timestamps\n                cancel_ts = pd.NaT if int(cancel) &gt; pd.Timestamp.max.value // 1_000_000 else pd.Timestamp(int(cancel), unit='ms', tz='UTC')\n                rows.append({\n                    'delivery_time': delivery_time,\n                    'side': 'sell',\n                    'order_id': int(order_id),\n                    'initial_id': int(initial_id),\n                    'start_time': pd.Timestamp(int(start), unit='ms', tz='UTC'),\n                    'cancel_time': cancel_ts,\n                    'price': price,\n                    'volume': volume,\n                    'is_forecast': bool(forecast),\n                    'cumulative_volume': cumulative_sell_volume\n                })\n\n            # Process buy orders (where we can sell to)\n            buy_ids = data['buy_ids']\n            buy_initial_ids = data['buy_initial_ids']\n            buy_starts = data['buy_starts']\n            buy_cancels = data['buy_cancels']\n            buy_prices = data['buy_prices']\n            buy_volumes = data['buy_volumes']\n            buy_forecasts = data['buy_forecasts']\n\n            cumulative_buy_volume = 0.0\n            for order_id, initial_id, start, cancel, price, volume, forecast in zip(\n                buy_ids, buy_initial_ids, buy_starts, buy_cancels, buy_prices, buy_volumes, buy_forecasts\n            ):\n                cumulative_buy_volume += volume\n                # Handle sentinel cancel time values (e.g., max int64) that overflow pandas timestamps\n                cancel_ts = pd.NaT if int(cancel) &gt; pd.Timestamp.max.value // 1_000_000 else pd.Timestamp(int(cancel), unit='ms', tz='UTC')\n                rows.append({\n                    'delivery_time': delivery_time,\n                    'side': 'buy',\n                    'order_id': int(order_id),\n                    'initial_id': int(initial_id),\n                    'start_time': pd.Timestamp(int(start), unit='ms', tz='UTC'),\n                    'cancel_time': cancel_ts,\n                    'price': price,\n                    'volume': volume,\n                    'is_forecast': bool(forecast),\n                    'cumulative_volume': cumulative_buy_volume\n                })\n\n        df = pd.DataFrame(rows)\n        return df\n\n    def reached_end_of_day(self, is_last: bool) -&gt; bool:\n        \"\"\"\n        Check if the order queue has reached the end for this day.\n\n        This function mirrors the logic of run_one_day(is_last) for checking\n        whether we've processed all available orders in the current batch.\n\n        Parameters\n        ----------\n        is_last : bool\n            Whether this is the last data batch (same semantics as run_one_day).\n            - If False: indicates more data-days will be loaded after this batch\n            - If True: indicates this is the final batch of data-days for this simulation\n\n        Returns\n        -------\n        bool\n            True if there are no more orders to process in the queue, False otherwise.\n\n        Notes\n        -----\n        - Returns True when orderQueue.hasNext() is False in C++\n        - The is_last parameter is kept for API consistency with run_one_day\n        - Use this to check if the simulation stopped because it ran out of orders\n          vs. stopping due to a stop time or stop hour\n\n        Example\n        -------\n        &gt;&gt;&gt; sim.run_one_day(is_last=False)\n        &gt;&gt;&gt; if sim.reached_end_of_day(is_last=False):\n        ...     print(\"Processed all orders in current batch, ready for next day's data\")\n        &gt;&gt;&gt; elif sim.has_stopped_at_stop_time():\n        ...     print(\"Stopped at specified stop time\")\n        \"\"\"\n        return self._sim_cpp.reachedEndOfDay(is_last)\n\n    def has_stopped_at_stop_time(self) -&gt; bool:\n        \"\"\"\n        Check if the simulation has stopped due to the stop time being reached. Is set to false again once we set a new stop time.\n\n        Returns\n        -------\n        bool\n            True if the simulation stopped because the last order's submission time\n            exceeded the set stop time. False otherwise.\n\n        Notes\n        -----\n        - This flag is set when the simulation stops due to a stop time set via set_stop_time()\n        - The flag is automatically reset when a new stop time is set\n        - Use this to determine if a simulation pause was due to the stop time condition\n\n        Example\n        -------\n        &gt;&gt;&gt; sim.set_stop_time(pd.Timestamp('2024-01-15 12:30:00', tz='UTC'))\n        &gt;&gt;&gt; sim.run_one_day(is_last=False)\n        &gt;&gt;&gt; if sim.has_stopped_at_stop_time():\n        ...     print(\"Simulation stopped at the specified stop time\")\n        \"\"\"\n        return self._sim_cpp.hasStoppedAtStopTime()\n\n    def get_last_order_placement_time(self) -&gt; pd.Timestamp:\n        \"\"\"\n        Get the simulation's current time (last processed order's placement time) as a UTC datetime.\n\n        This returns the internal `_lastOrder_placementTime` from the C++ simulation,\n        converted to a timezone-aware pandas Timestamp in UTC.\n\n        Returns\n        -------\n        pd.Timestamp\n            The timestamp of the last processed order's placement time in UTC.\n            Returns pd.NaT if no orders have been processed yet (time is at minimum int64 value).\n\n        Notes\n        -----\n        - The internal time is stored as milliseconds since epoch (Unix time)\n        - This is the time used by methods like get_limit_order_book_state() and solve()\n        - Before any orders are processed, this returns pd.NaT\n\n        Example\n        -------\n        &gt;&gt;&gt; sim.add_bin_to_orderqueue(\"path/to/data.bin\")\n        &gt;&gt;&gt; sim.run_one_day(is_last=False)\n        &gt;&gt;&gt; current_time = sim.get_last_order_placement_time()\n        &gt;&gt;&gt; print(current_time)\n        2024-01-15 12:30:45.123000+00:00\n        \"\"\"\n        time_ms = self._sim_cpp.getLastOrderPlacementTimeMs()\n\n        # Handle uninitialized state (minimum int64 value)\n        # In C++, it's initialized to std::numeric_limits&lt;int64_t&gt;::min()\n        if time_ms &lt;= -sys.maxsize:\n            return pd.NaT\n\n        # Convert milliseconds since epoch to UTC timestamp\n        return pd.Timestamp(time_ms, unit='ms', tz='UTC')\n    def get_next_order_start_time(self) -&gt; pd.Timestamp:\n        \"\"\"\n        Get the order queue's next order start time as a UTC datetime.\n\n        This peeks at the next order in the queue without consuming it, returning\n        its start time (placement time) converted to a timezone-aware pandas Timestamp in UTC.\n\n        Returns\n        -------\n        pd.Timestamp\n            The timestamp of the next order's start time in UTC.\n            Returns pd.NaT if no more orders are available in the queue.\n\n        Notes\n        -----\n        - The internal time is stored as milliseconds since epoch (Unix time)\n        - This does not advance the order queue; it only peeks at the next order\n        - Useful for determining when the next order will be processed\n\n        Example\n        -------\n        &gt;&gt;&gt; sim.add_bin_to_orderqueue(\"path/to/data.bin\")\n        &gt;&gt;&gt; next_time = sim.get_next_order_start_time()\n        &gt;&gt;&gt; print(next_time)\n        2024-01-15 12:30:45.123000+00:00\n        \"\"\"\n        time_ms = self._sim_cpp.getNextOrderStartTimeMs()\n\n        # Handle case when no next order exists (minimum int64 value)\n        if time_ms &lt;= -sys.maxsize:\n            return pd.NaT\n\n        # Convert milliseconds since epoch to UTC timestamp\n        return pd.Timestamp(time_ms, unit='ms', tz='UTC')\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.__init__","title":"<code>__init__(start_date, end_date, trading_start_date=None, storage_max=10.0, lin_deg_cost=4.0, loss_in=0.95, loss_out=0.95, trading_fee=0.09, num_stor_states=11, trading_delay=0, tec_delay=0, fixed_solve_time=0, solve_frequency=0.0, withdraw_max=5.0, inject_max=5.0, log_transactions=False, only_traverse_lob=False, cycle_limit=None, min_hot_queue_size=-1)</code>","text":"<p>Initialize a Simulation instance.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp</code> <p>The start datetime of the simulation, i.e. which products are loaded into the simulation. Must be timezone aware.</p> required <code>end_date</code> <code>Timestamp</code> <p>The end datetime of the simulation, i.e. which products are loaded into the simulation. Must be timezone aware.</p> required <code>trading_start_date</code> <code>Timestamp</code> <p>The start datetime of the trading, i.e. when the trading starts. Must be timezone aware. If None, the trading starts at the same time as the start_date.</p> <code>None</code> <code>storage_max</code> <code>float</code> <p>The maximum storage capacity of the storage unit (MWh). Default is 10.0.</p> <code>10.0</code> <code>lin_deg_cost</code> <code>float</code> <p>The linear degradation cost of the storage unit (\u20ac/MWh). Default is 4.0.</p> <code>4.0</code> <code>loss_in</code> <code>float</code> <p>The injection efficiency of the storage unit (0-1]. Default is 0.95.</p> <code>0.95</code> <code>loss_out</code> <code>float</code> <p>The withdrawal efficiency of the storage unit (0-1]. Default is 0.95.</p> <code>0.95</code> <code>trading_fee</code> <code>float</code> <p>The trading fee for the exchange (\u20ac/MWh). Default is 0.09.</p> <code>0.09</code> <code>num_stor_states</code> <code>int</code> <p>The number of storage states for dynamic programming. Default is 11.</p> <code>11</code> <code>trading_delay</code> <code>int</code> <p>The trading delay of the storage unit, i.e., when to start trading all new products after gate opening. (min, &gt;= 0 and &lt; 480 mins (8 hours)). Default is 0.</p> <code>0</code> <code>tec_delay</code> <code>int</code> <p>The technical delay of the storage unit (ms, &gt;= 0). Default is 0.</p> <code>0</code> <code>fixed_solve_time</code> <code>int</code> <p>The fixed solve time for dynamic programming (ms, &gt;= 0 or -1 for realistic solve times). Default is 0.</p> <code>0</code> <code>solve_frequency</code> <code>float</code> <p>The frequency at which the dynamic programming solver is run (min). Default is 0.0.</p> <code>0.0</code> <code>withdraw_max</code> <code>float</code> <p>The maximum withdrawal power of the storage unit (MW). Default is 5.0.</p> <code>5.0</code> <code>inject_max</code> <code>float</code> <p>The maximum injection power of the storage unit (MW). Default is 5.0.</p> <code>5.0</code> <code>log_transactions</code> <code>bool</code> <p>If True, we run the simulation only to log transactions data of the market, no optimization is performed. Default is False.</p> <code>False</code> <code>only_traverse_lob</code> <p>Whether to only traverse the LOB and not call any DP solves. (bool, default: False)</p> <code>False</code> <code>cycle_limit</code> <code>float</code> <p>The limit on the number of cycles per Berlin-time day. Setting it comes at a cost in terms of solve time. (float, &gt; 0). Default is None, where no cycle limit is enforced.</p> <code>None</code> <code>min_hot_queue_size</code> <code>int</code> <p>The minimum number of orders to keep in the hot cache for each order queue. (int, &gt; 0, or -1 to disable and use only volume-based caching, default: -1)</p> <code>-1</code> Source code in <code>bitepy/simulation.py</code> <pre><code>def __init__(self, start_date: pd.Timestamp, end_date: pd.Timestamp,\n             trading_start_date: pd.Timestamp=None,\n             storage_max=10.,\n             lin_deg_cost=4.,\n             loss_in=0.95,\n             loss_out=0.95,\n             trading_fee=0.09,\n             num_stor_states=11,\n             trading_delay=0,\n             tec_delay=0,\n             fixed_solve_time=0,\n             solve_frequency=0.,\n             withdraw_max=5.,\n             inject_max=5.,\n             log_transactions=False,\n             only_traverse_lob=False,\n             cycle_limit: float = None,\n             min_hot_queue_size: int = -1,):\n            #  forecast_horizon_start=10*60,\n            #  forecast_horizon_end=75):\n    \"\"\"\n    Initialize a Simulation instance.\n\n    Args:\n        start_date (pd.Timestamp): The start datetime of the simulation, i.e. which products are loaded into the simulation. Must be timezone aware.\n        end_date (pd.Timestamp): The end datetime of the simulation, i.e. which products are loaded into the simulation. Must be timezone aware.\n        trading_start_date (pd.Timestamp, optional): The start datetime of the trading, i.e. when the trading starts. Must be timezone aware. If None, the trading starts at the same time as the start_date.\n        storage_max (float, optional): The maximum storage capacity of the storage unit (MWh). Default is 10.0.\n        lin_deg_cost (float, optional): The linear degradation cost of the storage unit (\u20ac/MWh). Default is 4.0.\n        loss_in (float, optional): The injection efficiency of the storage unit (0-1]. Default is 0.95.\n        loss_out (float, optional): The withdrawal efficiency of the storage unit (0-1]. Default is 0.95.\n        trading_fee (float, optional): The trading fee for the exchange (\u20ac/MWh). Default is 0.09.\n        num_stor_states (int, optional): The number of storage states for dynamic programming. Default is 11.\n        trading_delay (int, optional): The trading delay of the storage unit, i.e., when to start trading all new products after gate opening. (min, &gt;= 0 and &lt; 480 mins (8 hours)). Default is 0.\n        tec_delay (int, optional): The technical delay of the storage unit (ms, &gt;= 0). Default is 0.\n        fixed_solve_time (int, optional): The fixed solve time for dynamic programming (ms, &gt;= 0 or -1 for realistic solve times). Default is 0.\n        solve_frequency (float, optional): The frequency at which the dynamic programming solver is run (min). Default is 0.0.\n        withdraw_max (float, optional): The maximum withdrawal power of the storage unit (MW). Default is 5.0.\n        inject_max (float, optional): The maximum injection power of the storage unit (MW). Default is 5.0.\n        log_transactions (bool, optional): If True, we run the simulation only to log transactions data of the market, no optimization is performed. Default is False.\n        only_traverse_lob: Whether to only traverse the LOB and not call any DP solves. (bool, default: False)\n        cycle_limit: The limit on the number of cycles per Berlin-time day. Setting it comes at a cost in terms of solve time. (float, &gt; 0). Default is None, where no cycle limit is enforced.\n        min_hot_queue_size: The minimum number of orders to keep in the hot cache for each order queue. (int, &gt; 0, or -1 to disable and use only volume-based caching, default: -1)\n    \"\"\"\n    # forecast_horizon_start (int, optional): The start of the forecast horizon (min). Default is 600.\n    # forecast_horizon_end (int, optional): The end of the forecast horizon (min). Default is 75.\n\n    # write all the assertions\n    if start_date &gt;= end_date:\n        raise ValueError(\"start_date must be before end_date\")\n    if trading_start_date is None:\n        trading_start_date = start_date\n    if trading_start_date &gt;= end_date:\n        raise ValueError(\"trading_start_date must be before end_date\")\n    if storage_max &lt; 0:\n        raise ValueError(\"storage_max must be &gt;= 0\")\n    if lin_deg_cost &lt; 0:\n        raise ValueError(\"lin_deg_cost must be &gt;= 0\")\n    if loss_in &lt; 0 or loss_in &gt; 1:\n        raise ValueError(\"loss_in must be in [0, 1]\")\n    if loss_out &lt; 0 or loss_out &gt; 1:\n        raise ValueError(\"loss_out must be in [0,1]\")\n    if trading_fee &lt; 0:\n        raise ValueError(\"trading_fee must be &gt;= 0\")\n    if num_stor_states &lt;= 0:\n        raise ValueError(\"num_stor_states must be &gt; 0\")\n    if tec_delay &lt; 0:\n        raise ValueError(\"tec_delay must be &gt;= 0\")\n    if fixed_solve_time &lt; 0:\n        if fixed_solve_time != -1:\n            raise ValueError(\"fixed_solve_time must be &gt;= 0 (or -1 for realistic solve times)\")\n    if solve_frequency &lt; 0:\n        raise ValueError(\"solve_frequency must be &gt;= 0\")\n    if withdraw_max &lt;= 0:\n        raise ValueError(\"withdraw_max must be &gt; 0\")\n    if inject_max &lt;= 0:\n        raise ValueError(\"inject_max must be &gt; 0\")\n    if trading_delay &lt; 0 or trading_delay &gt;= 8*60:\n        raise ValueError(\"trading_delay must be &gt;= 0 and &lt; 480 mins (8 hours)\")\n    if cycle_limit is not None:\n        if cycle_limit &lt;= 0:\n            raise ValueError(\"cycle_limit must be &gt; 0 if provided\")\n    if min_hot_queue_size &lt;= 0 and min_hot_queue_size != -1:\n        raise ValueError(\"min_hot_queue_size must be &gt; 0 or -1 (to disable)\")\n    # if forecast_horizon_start &lt; 0:\n    #     raise ValueError(\"forecast_horizon_start must be &gt;= 0\")\n    # if forecast_horizon_end &lt; 0:\n    #     raise ValueError(\"forecast_horizon_end must be &gt;= 0\")\n    # if forecast_horizon_start &lt;= forecast_horizon_end:\n    #     raise ValueError(\"forecast_horizon_start must larger than forecast_horizon_end\")\n\n    self._sim_cpp = Simulation_cpp()\n\n    self._sim_cpp.params.storageMax = storage_max\n    self._sim_cpp.params.linDegCost = lin_deg_cost\n    self._sim_cpp.params.lossIn = loss_in\n    self._sim_cpp.params.lossOut = loss_out\n    self._sim_cpp.params.tradingFee = trading_fee\n    self._sim_cpp.params.numStorStates = num_stor_states\n    self._sim_cpp.params.pingDelay = tec_delay\n    self._sim_cpp.params.fixedSolveTime = fixed_solve_time\n    self._sim_cpp.params.dpFreq = solve_frequency\n    self._sim_cpp.params.withdrawMax = withdraw_max\n    self._sim_cpp.params.injectMax = inject_max\n    self._sim_cpp.params.minuteDelay = trading_delay\n    self._sim_cpp.params.logTransactions = log_transactions\n    self._sim_cpp.params.onlyTraverseLOB = only_traverse_lob\n    if cycle_limit is not None:\n        self._sim_cpp.params.cycleLimit = float(cycle_limit)\n    self._sim_cpp.params.minHotQueueSize = min_hot_queue_size\n    # self._sim_cpp.params.foreHorizonStart = forecast_horizon_start\n    # self._sim_cpp.params.foreHorizonEnd = forecast_horizon_end\n\n    # Set start and end date\n    if start_date &gt;= end_date:\n        raise ValueError(\"start_date must be before end_date\")\n    if start_date.tzinfo is None:\n        raise ValueError(\"start_date must be timezone aware\")\n    start_date = start_date.astimezone(pytz.utc)\n    self._sim_cpp.params.startMonth = start_date.month\n    self._sim_cpp.params.startDay = start_date.day\n    self._sim_cpp.params.startYear = start_date.year\n    self._sim_cpp.params.startHour = start_date.hour\n    self._sim_cpp.params.startMinute = start_date.minute\n    if end_date.tzinfo is None:\n        raise ValueError(\"end_date must be timezone aware\")\n    end_date = end_date.astimezone(pytz.utc)\n    self._sim_cpp.params.endMonth = end_date.month\n    self._sim_cpp.params.endDay = end_date.day\n    self._sim_cpp.params.endYear = end_date.year\n    self._sim_cpp.params.endHour = end_date.hour\n    self._sim_cpp.params.endMinute = end_date.minute\n\n    # Set trading start date\n    if trading_start_date.tzinfo is None:\n        raise ValueError(\"trading_start_date must be timezone aware\")\n    trading_start_date = trading_start_date.astimezone(pytz.utc)\n    self._sim_cpp.params.tradingStartMonth = trading_start_date.month\n    self._sim_cpp.params.tradingStartDay = trading_start_date.day\n    self._sim_cpp.params.tradingStartYear = trading_start_date.year\n    self._sim_cpp.params.tradingStartHour = trading_start_date.hour\n    self._sim_cpp.params.tradingStartMinute = trading_start_date.minute\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.add_bin_to_orderqueue","title":"<code>add_bin_to_orderqueue(bin_data)</code>","text":"<p>Add an order binary file to the simulation's order queue.</p> <p>Parameters:</p> Name Type Description Default <code>bin_data</code> <code>str</code> <p>The path to the order binary file.</p> required Source code in <code>bitepy/simulation.py</code> <pre><code>def add_bin_to_orderqueue(self, bin_data: str):\n    \"\"\"\n    Add an order binary file to the simulation's order queue.\n\n    Args:\n        bin_data (str): The path to the order binary file.\n    \"\"\"\n    self._sim_cpp.addOrderQueueFromBin(bin_data)\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.add_df_to_orderqueue","title":"<code>add_df_to_orderqueue(df)</code>","text":"<p>Add a DataFrame of orders to the simulation's order queue.</p> <p>The DataFrame must have the same columns as the saved CSV files, with timestamps in UTC (seconds and milliseconds).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame containing the orders to be added.</p> required Processing Steps <ul> <li>Validate that the timestamp columns ('start', 'transaction', 'validity') are timezone aware.</li> <li>Ensure that all timestamps are in the same timezone.</li> <li>Convert all timestamps to UTC and format them in ISO 8601.</li> </ul> Source code in <code>bitepy/simulation.py</code> <pre><code>def add_df_to_orderqueue(self, df: pd.DataFrame):\n    \"\"\"\n    Add a DataFrame of orders to the simulation's order queue.\n\n    The DataFrame must have the same columns as the saved CSV files, with timestamps in UTC\n    (seconds and milliseconds).\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing the orders to be added.\n\n    Processing Steps:\n        - Validate that the timestamp columns ('start', 'transaction', 'validity') are timezone aware.\n        - Ensure that all timestamps are in the same timezone.\n        - Convert all timestamps to UTC and format them in ISO 8601.\n    \"\"\"\n    if (df[\"start\"].dt.tz is None and df[\"transaction\"].dt.tz is None and df[\"validity\"].dt.tz is None):\n        raise ValueError(\"All timestamps of input df must be timezone aware\")\n    if not (df[\"start\"].dt.tz == df[\"transaction\"].dt.tz and df[\"start\"].dt.tz == df[\"validity\"].dt.tz):\n        raise ValueError(\"All timestamps of input df must be in the same timezone\")\n\n    df[\"start\"] = df[\"start\"].dt.tz_convert(\"UTC\")\n    df[\"transaction\"] = df[\"transaction\"].dt.tz_convert(\"UTC\")\n    df[\"validity\"] = df[\"validity\"].dt.tz_convert(\"UTC\")\n    df[\"start\"] = df[\"start\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n    df[\"transaction\"] = df[\"transaction\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n    df[\"validity\"] = df[\"validity\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n\n    ids = df['id'].to_numpy(dtype=np.int64).tolist()\n    initials = df['initial'].to_numpy(dtype=np.int64).tolist()\n    sides = df['side'].to_numpy(dtype='str').tolist()\n    starts = df['start'].to_numpy(dtype='str').tolist()\n    transactions = df['transaction'].to_numpy(dtype='str').tolist()\n    validities = df['validity'].to_numpy(dtype='str').tolist()\n    prices = df['price'].to_numpy(dtype=np.float64).tolist()\n    quantities = df['quantity'].to_numpy(dtype=np.float64).tolist()\n\n    self._sim_cpp.addOrderQueueFromPandas(ids, initials, sides, starts, transactions, validities, prices, quantities)\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.get_data_bins_for_each_day","title":"<code>get_data_bins_for_each_day(base_path, start_date, end_date)</code>","text":"<p>Generate a list of file paths for binary order book data for each day within a date range.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str</code> <p>The base directory path where the binary files are stored.</p> required <code>start_date</code> <code>Timestamp</code> <p>The start date of the range.</p> required <code>end_date</code> <code>Timestamp</code> <p>The end date of the range.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of file paths for each day's binary order book file.</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def get_data_bins_for_each_day(self, base_path: str, start_date: pd.Timestamp, end_date: pd.Timestamp):\n    \"\"\"\n    Generate a list of file paths for binary order book data for each day within a date range.\n\n    Args:\n        base_path (str): The base directory path where the binary files are stored.\n        start_date (pd.Timestamp): The start date of the range.\n        end_date (pd.Timestamp): The end date of the range.\n\n    Returns:\n        list: A list of file paths for each day's binary order book file.\n    \"\"\"\n    # convert dates to utc time\n    start_date_berlin = start_date.tz_convert('Europe/Berlin') # convert to tz in which the lob files are segemented\n    end_date_berlin = end_date.tz_convert('Europe/Berlin') # convert to tz in which the lob files are segemented\n\n    # round up to midnight\n    end_date_berlin_round_up = end_date_berlin.replace(hour=23, minute=59, second=59)\n\n    base_path = os.path.join(base_path, '')\n    base_path += \"orderbook_\"\n\n    # Generate paths for each day within the date range\n    paths = []\n\n    current_date = start_date_berlin - timedelta(days=1) # include the day before the start date to ensure that all orders submitted with delivery on first day are included\n    while current_date &lt; end_date_berlin_round_up:\n        path = f\"{base_path}{current_date.strftime('%Y-%m-%d')}.bin\"\n        paths.append(path)\n        current_date += timedelta(days=1)\n\n    return paths\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.get_last_order_placement_time","title":"<code>get_last_order_placement_time()</code>","text":"<p>Get the simulation's current time (last processed order's placement time) as a UTC datetime.</p> <p>This returns the internal <code>_lastOrder_placementTime</code> from the C++ simulation, converted to a timezone-aware pandas Timestamp in UTC.</p>"},{"location":"simulation/#bitepy.Simulation.get_last_order_placement_time--returns","title":"Returns","text":"<p>pd.Timestamp     The timestamp of the last processed order's placement time in UTC.     Returns pd.NaT if no orders have been processed yet (time is at minimum int64 value).</p>"},{"location":"simulation/#bitepy.Simulation.get_last_order_placement_time--notes","title":"Notes","text":"<ul> <li>The internal time is stored as milliseconds since epoch (Unix time)</li> <li>This is the time used by methods like get_limit_order_book_state() and solve()</li> <li>Before any orders are processed, this returns pd.NaT</li> </ul>"},{"location":"simulation/#bitepy.Simulation.get_last_order_placement_time--example","title":"Example","text":"<p>sim.add_bin_to_orderqueue(\"path/to/data.bin\") sim.run_one_day(is_last=False) current_time = sim.get_last_order_placement_time() print(current_time) 2024-01-15 12:30:45.123000+00:00</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def get_last_order_placement_time(self) -&gt; pd.Timestamp:\n    \"\"\"\n    Get the simulation's current time (last processed order's placement time) as a UTC datetime.\n\n    This returns the internal `_lastOrder_placementTime` from the C++ simulation,\n    converted to a timezone-aware pandas Timestamp in UTC.\n\n    Returns\n    -------\n    pd.Timestamp\n        The timestamp of the last processed order's placement time in UTC.\n        Returns pd.NaT if no orders have been processed yet (time is at minimum int64 value).\n\n    Notes\n    -----\n    - The internal time is stored as milliseconds since epoch (Unix time)\n    - This is the time used by methods like get_limit_order_book_state() and solve()\n    - Before any orders are processed, this returns pd.NaT\n\n    Example\n    -------\n    &gt;&gt;&gt; sim.add_bin_to_orderqueue(\"path/to/data.bin\")\n    &gt;&gt;&gt; sim.run_one_day(is_last=False)\n    &gt;&gt;&gt; current_time = sim.get_last_order_placement_time()\n    &gt;&gt;&gt; print(current_time)\n    2024-01-15 12:30:45.123000+00:00\n    \"\"\"\n    time_ms = self._sim_cpp.getLastOrderPlacementTimeMs()\n\n    # Handle uninitialized state (minimum int64 value)\n    # In C++, it's initialized to std::numeric_limits&lt;int64_t&gt;::min()\n    if time_ms &lt;= -sys.maxsize:\n        return pd.NaT\n\n    # Convert milliseconds since epoch to UTC timestamp\n    return pd.Timestamp(time_ms, unit='ms', tz='UTC')\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.get_limit_order_book_state","title":"<code>get_limit_order_book_state(max_action=None, return_dict=False)</code>","text":"<p>Get the current state of all active limit order books at the last-set stop time.</p> <p>This method returns, for each tradable product (delivery hour), the individual limit orders in the buy and sell queues with all their attributes, up to a cumulative volume of max_action. The query time is automatically set to the last-set stop time.</p>"},{"location":"simulation/#bitepy.Simulation.get_limit_order_book_state--parameters","title":"Parameters","text":"<p>max_action : float, optional     The maximum cumulative volume to query in MW (= MWh for 1-hour products).     If None (default), uses inject_max + withdraw_max from simulation parameters.     Must be &gt; 0 if specified. return_dict : bool, optional     Whether to return the limit order book state as a dictionary.     If True, the return value is a dictionary with the delivery time as the key and the limit order book state as the value.     If False (default), the return value is a DataFrame.</p>"},{"location":"simulation/#bitepy.Simulation.get_limit_order_book_state--returns","title":"Returns","text":"<p>pd.DataFrame or dict     A DataFrame containing the limit orders with the following columns:         - delivery_time: The delivery time of the product (UTC timestamp)         - side: 'sell' or 'buy' (sell orders are where you can buy from, buy orders are where you can sell to)         - order_id: The unique order ID         - initial_id: The initial order ID (for tracking order modifications)         - start_time: When the order was placed (UTC timestamp)         - cancel_time: When the order expires (UTC timestamp)         - price: The limit order price in EUR/MWh         - volume: The order volume in MWh         - is_forecast: Whether this is a forecast order (bool)         - cumulative_volume: The cumulative volume up to and including this order in MWh</p>"},{"location":"simulation/#bitepy.Simulation.get_limit_order_book_state--notes","title":"Notes","text":"<ul> <li>Uses the simulation's internal current time (_lastOrder_placementTime)</li> <li>Sell orders are sorted by ascending price (cheapest first = best for buying)</li> <li>Buy orders are sorted by descending price (highest first = best for selling)</li> <li>Orders are filtered to exclude expired orders at the query time</li> <li>Cumulative volume stops at max_action (default: inject_max + withdraw_max)</li> <li>Each row represents one limit order in the order book</li> </ul>"},{"location":"simulation/#bitepy.Simulation.get_limit_order_book_state--example","title":"Example","text":"Source code in <code>bitepy/simulation.py</code> <pre><code>def get_limit_order_book_state(self, max_action: float = None, return_dict: bool = False):\n    \"\"\"\n    Get the current state of all active limit order books at the last-set stop time.\n\n    This method returns, for each tradable product (delivery hour), the individual limit orders\n    in the buy and sell queues with all their attributes, up to a cumulative volume of max_action.\n    The query time is automatically set to the last-set stop time.\n\n    Parameters\n    ----------\n    max_action : float, optional\n        The maximum cumulative volume to query in MW (= MWh for 1-hour products).\n        If None (default), uses inject_max + withdraw_max from simulation parameters.\n        Must be &gt; 0 if specified.\n    return_dict : bool, optional\n        Whether to return the limit order book state as a dictionary.\n        If True, the return value is a dictionary with the delivery time as the key and the limit order book state as the value.\n        If False (default), the return value is a DataFrame.\n\n    Returns\n    -------\n    pd.DataFrame or dict\n        A DataFrame containing the limit orders with the following columns:\n            - delivery_time: The delivery time of the product (UTC timestamp)\n            - side: 'sell' or 'buy' (sell orders are where you can buy from, buy orders are where you can sell to)\n            - order_id: The unique order ID\n            - initial_id: The initial order ID (for tracking order modifications)\n            - start_time: When the order was placed (UTC timestamp)\n            - cancel_time: When the order expires (UTC timestamp)\n            - price: The limit order price in EUR/MWh\n            - volume: The order volume in MWh\n            - is_forecast: Whether this is a forecast order (bool)\n            - cumulative_volume: The cumulative volume up to and including this order in MWh\n\n    Notes\n    -----\n    - Uses the simulation's internal current time (_lastOrder_placementTime)\n    - Sell orders are sorted by ascending price (cheapest first = best for buying)\n    - Buy orders are sorted by descending price (highest first = best for selling)\n    - Orders are filtered to exclude expired orders at the query time\n    - Cumulative volume stops at max_action (default: inject_max + withdraw_max)\n    - Each row represents one limit order in the order book\n\n    Example\n    -------\n    &gt;&gt;&gt; # Query order book state with default max_action (inject_max + withdraw_max)\n    &gt;&gt;&gt; lob_state = sim.get_limit_order_book_state()\n    &gt;&gt;&gt; # Query with custom max_action\n    &gt;&gt;&gt; lob_state = sim.get_limit_order_book_state(max_action=20.0)\n    &gt;&gt;&gt; # Filter to see sell orders for a specific product\n    &gt;&gt;&gt; product_sells = lob_state[(lob_state['delivery_time'] == some_time) &amp; (lob_state['side'] == 'sell')]\n    &gt;&gt;&gt; # See the best (cheapest) sell price\n    &gt;&gt;&gt; best_sell_price = product_sells.iloc[0]['price']\n    &gt;&gt;&gt; # Filter to see only forecast orders\n    &gt;&gt;&gt; forecast_orders = lob_state[lob_state['is_forecast'] == True]\n    \"\"\"\n    # Determine max_action value\n    if max_action is None:\n        # Use inject_max + withdraw_max as default\n        max_action_value = self._sim_cpp.params.injectMax + self._sim_cpp.params.withdrawMax\n    else:\n        # Validate that max_action is positive\n        if max_action &lt;= 0:\n            raise ValueError(\"max_action must be &gt; 0\")\n        max_action_value = max_action\n\n    # Call C++ function (uses simulation's current time internally)\n    lob_state_dict = self._sim_cpp.getLimitOrderBookState(max_action_value)\n\n    if return_dict:\n        return lob_state_dict\n\n    # Convert to DataFrame\n    rows = []\n    for delivery_time_ms, data in lob_state_dict.items():\n        delivery_time = pd.Timestamp(delivery_time_ms, unit='ms', tz='UTC')\n\n        # Process sell orders (where we can buy from)\n        sell_ids = data['sell_ids']\n        sell_initial_ids = data['sell_initial_ids']\n        sell_starts = data['sell_starts']\n        sell_cancels = data['sell_cancels']\n        sell_prices = data['sell_prices']\n        sell_volumes = data['sell_volumes']\n        sell_forecasts = data['sell_forecasts']\n\n        cumulative_sell_volume = 0.0\n        for order_id, initial_id, start, cancel, price, volume, forecast in zip(\n            sell_ids, sell_initial_ids, sell_starts, sell_cancels, sell_prices, sell_volumes, sell_forecasts\n        ):\n            cumulative_sell_volume += volume\n            # Handle sentinel cancel time values (e.g., max int64) that overflow pandas timestamps\n            cancel_ts = pd.NaT if int(cancel) &gt; pd.Timestamp.max.value // 1_000_000 else pd.Timestamp(int(cancel), unit='ms', tz='UTC')\n            rows.append({\n                'delivery_time': delivery_time,\n                'side': 'sell',\n                'order_id': int(order_id),\n                'initial_id': int(initial_id),\n                'start_time': pd.Timestamp(int(start), unit='ms', tz='UTC'),\n                'cancel_time': cancel_ts,\n                'price': price,\n                'volume': volume,\n                'is_forecast': bool(forecast),\n                'cumulative_volume': cumulative_sell_volume\n            })\n\n        # Process buy orders (where we can sell to)\n        buy_ids = data['buy_ids']\n        buy_initial_ids = data['buy_initial_ids']\n        buy_starts = data['buy_starts']\n        buy_cancels = data['buy_cancels']\n        buy_prices = data['buy_prices']\n        buy_volumes = data['buy_volumes']\n        buy_forecasts = data['buy_forecasts']\n\n        cumulative_buy_volume = 0.0\n        for order_id, initial_id, start, cancel, price, volume, forecast in zip(\n            buy_ids, buy_initial_ids, buy_starts, buy_cancels, buy_prices, buy_volumes, buy_forecasts\n        ):\n            cumulative_buy_volume += volume\n            # Handle sentinel cancel time values (e.g., max int64) that overflow pandas timestamps\n            cancel_ts = pd.NaT if int(cancel) &gt; pd.Timestamp.max.value // 1_000_000 else pd.Timestamp(int(cancel), unit='ms', tz='UTC')\n            rows.append({\n                'delivery_time': delivery_time,\n                'side': 'buy',\n                'order_id': int(order_id),\n                'initial_id': int(initial_id),\n                'start_time': pd.Timestamp(int(start), unit='ms', tz='UTC'),\n                'cancel_time': cancel_ts,\n                'price': price,\n                'volume': volume,\n                'is_forecast': bool(forecast),\n                'cumulative_volume': cumulative_buy_volume\n            })\n\n    df = pd.DataFrame(rows)\n    return df\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.get_limit_order_book_state--query-order-book-state-with-default-max_action-inject_max-withdraw_max","title":"Query order book state with default max_action (inject_max + withdraw_max)","text":"<p>lob_state = sim.get_limit_order_book_state()</p>"},{"location":"simulation/#bitepy.Simulation.get_limit_order_book_state--query-with-custom-max_action","title":"Query with custom max_action","text":"<p>lob_state = sim.get_limit_order_book_state(max_action=20.0)</p>"},{"location":"simulation/#bitepy.Simulation.get_limit_order_book_state--filter-to-see-sell-orders-for-a-specific-product","title":"Filter to see sell orders for a specific product","text":"<p>product_sells = lob_state[(lob_state['delivery_time'] == some_time) &amp; (lob_state['side'] == 'sell')]</p>"},{"location":"simulation/#bitepy.Simulation.get_limit_order_book_state--see-the-best-cheapest-sell-price","title":"See the best (cheapest) sell price","text":"<p>best_sell_price = product_sells.iloc[0]['price']</p>"},{"location":"simulation/#bitepy.Simulation.get_limit_order_book_state--filter-to-see-only-forecast-orders","title":"Filter to see only forecast orders","text":"<p>forecast_orders = lob_state[lob_state['is_forecast'] == True]</p>"},{"location":"simulation/#bitepy.Simulation.get_limit_order_matches","title":"<code>get_limit_order_matches()</code>","text":"<p>Get the limit order matches collected during simulation using the forecast tracking mechanism.</p> <p>This method retrieves match information for submitted limit orders that were processed during the simulation run. The orders are tracked using the forecast flag system.</p> Processing Steps <ul> <li>Retrieves match data from the C++ simulation backend.</li> <li>Converts timestamps to timezone-aware datetime objects.</li> <li>Clears the internal match storage after retrieval.</li> </ul> <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing information about which limit orders were matched against which existing orders. The DataFrame contains the following columns:     - submitted_order_id: The ID of the submitted limit order.     - matched_order_id: The ID of the existing order that was matched.     - match_timestamp: The timestamp when the match occurred.     - delivery_hour: The delivery hour for the matched order.     - match_price: The price at which the orders were matched, i.e., the price of the existing (partially) matched order (\u20ac/MWh).     - match_volume: The volume that was matched (MWh).     - submitted_order_side: The side of the submitted order ('buy' or 'sell').     - existing_order_side: The side of the existing order ('buy' or 'sell').</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def get_limit_order_matches(self):\n    \"\"\"\n    Get the limit order matches collected during simulation using the forecast tracking mechanism.\n\n    This method retrieves match information for submitted limit orders that were processed\n    during the simulation run. The orders are tracked using the forecast flag system.\n\n    Processing Steps:\n        - Retrieves match data from the C++ simulation backend.\n        - Converts timestamps to timezone-aware datetime objects.\n        - Clears the internal match storage after retrieval.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing information about which limit orders were matched against which existing orders.\n            The DataFrame contains the following columns:\n                - submitted_order_id: The ID of the submitted limit order.\n                - matched_order_id: The ID of the existing order that was matched.\n                - match_timestamp: The timestamp when the match occurred.\n                - delivery_hour: The delivery hour for the matched order.\n                - match_price: The price at which the orders were matched, i.e., the price of the existing (partially) matched order (\u20ac/MWh).\n                - match_volume: The volume that was matched (MWh).\n                - submitted_order_side: The side of the submitted order ('buy' or 'sell').\n                - existing_order_side: The side of the existing order ('buy' or 'sell').\n    \"\"\"\n    matches = self._sim_cpp.getLimitOrderMatches()\n    matches_df = pd.DataFrame(matches)\n\n    # Convert timestamps to datetime if not empty\n    if not matches_df.empty:\n        matches_df[\"match_timestamp\"] = pd.to_datetime(matches_df[\"match_timestamp\"], utc=True)\n        matches_df[\"delivery_hour\"] = pd.to_datetime(matches_df[\"delivery_hour\"], utc=True)\n    else:\n        print(\"No limit order matches to return.\")\n\n    self._sim_cpp.clearLimitOrderMatches() # clear existing matches\n\n    return matches_df\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.get_logs","title":"<code>get_logs()</code>","text":"<p>Retrieve the logs generated by the simulation.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing simulation logs with the following keys: - decision_record: Final simulation schedule. - price_record: CID price data over the simulation duration. - accepted_orders: Limit orders accepted by the RI. - executed_orders: Orders sent to the exchange by the RI. - killed_orders: Orders that were missed at the exchange.</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def get_logs(self):\n    \"\"\"\n    Retrieve the logs generated by the simulation.\n\n    Returns:\n        dict: A dictionary containing simulation logs with the following keys:\n            - decision_record: Final simulation schedule.\n            - price_record: CID price data over the simulation duration.\n            - accepted_orders: Limit orders accepted by the RI.\n            - executed_orders: Orders sent to the exchange by the RI.\n            - killed_orders: Orders that were missed at the exchange.\n    \"\"\"\n    # - forecast_orders: Orders virtually traded against the forecast.\n    # - balancing_orders: Orders that would have incurred payments to the TSO.\n    decision_record, price_record, accepted_orders, executed_orders, forecast_orders, killed_orders, balancing_orders = self._sim_cpp.getLogs()\n    decision_record = pd.DataFrame(decision_record)\n    price_record = pd.DataFrame(price_record)\n    accepted_orders = pd.DataFrame(accepted_orders)\n    executed_orders = pd.DataFrame(executed_orders)\n    forecast_orders = pd.DataFrame(forecast_orders)\n    killed_orders = pd.DataFrame(killed_orders)\n    balancing_orders = pd.DataFrame(balancing_orders)\n\n    if not decision_record.empty:\n        decision_record[\"hour\"] = pd.to_datetime(decision_record[\"hour\"], utc=True)\n        decision_record[\"cycles\"] = np.round(decision_record[\"cycles\"].astype(float), 2)\n    if not price_record.empty:\n        price_record[\"hour\"] = pd.to_datetime(price_record[\"hour\"], utc=True)\n    if not accepted_orders.empty:\n        accepted_orders[\"time\"] = pd.to_datetime(accepted_orders[\"time\"], utc=True)\n        accepted_orders[\"start\"] = pd.to_datetime(accepted_orders[\"start\"], utc=True)\n        accepted_orders[\"cancel\"] = pd.to_datetime(accepted_orders[\"cancel\"], utc=True)\n        accepted_orders[\"delivery\"] = pd.to_datetime(accepted_orders[\"delivery\"], utc=True)\n    if not executed_orders.empty:\n        executed_orders[\"time\"] = pd.to_datetime(executed_orders[\"time\"], utc=True)\n        executed_orders[\"last_solve_time\"] = pd.to_datetime(executed_orders[\"last_solve_time\"], utc=True)\n        executed_orders[\"hour\"] = pd.to_datetime(executed_orders[\"hour\"], utc=True)\n    if not forecast_orders.empty:\n        forecast_orders[\"time\"] = pd.to_datetime(forecast_orders[\"time\"], utc=True)\n        forecast_orders[\"last_solve_time\"] = pd.to_datetime(forecast_orders[\"last_solve_time\"], utc=True)\n        forecast_orders[\"hour\"] = pd.to_datetime(forecast_orders[\"hour\"], utc=True)\n    if not killed_orders.empty:\n        killed_orders[\"time\"] = pd.to_datetime(killed_orders[\"time\"], utc=True)\n        killed_orders[\"last_solve_time\"] = pd.to_datetime(killed_orders[\"last_solve_time\"], utc=True)\n        killed_orders[\"hour\"] = pd.to_datetime(killed_orders[\"hour\"], utc=True)\n    if not balancing_orders.empty:\n        balancing_orders[\"time\"] = pd.to_datetime(balancing_orders[\"time\"], utc=True)\n        balancing_orders[\"hour\"] = pd.to_datetime(balancing_orders[\"hour\"], utc=True)\n\n    logs = {\n        \"decision_record\": pd.DataFrame(decision_record, index=None),\n        \"price_record\": pd.DataFrame(price_record, index=None),\n        \"accepted_orders\": pd.DataFrame(accepted_orders, index=None),\n        \"executed_orders\": pd.DataFrame(executed_orders, index=None),\n        # \"forecast_orders\": pd.DataFrame(forecast_orders, index=None), # removed for later versions of the code\n        \"killed_orders\": pd.DataFrame(killed_orders, index=None),\n        # \"balancing_orders\": pd.DataFrame(balancing_orders, index=None), # removed for later versions of the code\n    }\n    return logs\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.get_next_order_start_time","title":"<code>get_next_order_start_time()</code>","text":"<p>Get the order queue's next order start time as a UTC datetime.</p> <p>This peeks at the next order in the queue without consuming it, returning its start time (placement time) converted to a timezone-aware pandas Timestamp in UTC.</p>"},{"location":"simulation/#bitepy.Simulation.get_next_order_start_time--returns","title":"Returns","text":"<p>pd.Timestamp     The timestamp of the next order's start time in UTC.     Returns pd.NaT if no more orders are available in the queue.</p>"},{"location":"simulation/#bitepy.Simulation.get_next_order_start_time--notes","title":"Notes","text":"<ul> <li>The internal time is stored as milliseconds since epoch (Unix time)</li> <li>This does not advance the order queue; it only peeks at the next order</li> <li>Useful for determining when the next order will be processed</li> </ul>"},{"location":"simulation/#bitepy.Simulation.get_next_order_start_time--example","title":"Example","text":"<p>sim.add_bin_to_orderqueue(\"path/to/data.bin\") next_time = sim.get_next_order_start_time() print(next_time) 2024-01-15 12:30:45.123000+00:00</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def get_next_order_start_time(self) -&gt; pd.Timestamp:\n    \"\"\"\n    Get the order queue's next order start time as a UTC datetime.\n\n    This peeks at the next order in the queue without consuming it, returning\n    its start time (placement time) converted to a timezone-aware pandas Timestamp in UTC.\n\n    Returns\n    -------\n    pd.Timestamp\n        The timestamp of the next order's start time in UTC.\n        Returns pd.NaT if no more orders are available in the queue.\n\n    Notes\n    -----\n    - The internal time is stored as milliseconds since epoch (Unix time)\n    - This does not advance the order queue; it only peeks at the next order\n    - Useful for determining when the next order will be processed\n\n    Example\n    -------\n    &gt;&gt;&gt; sim.add_bin_to_orderqueue(\"path/to/data.bin\")\n    &gt;&gt;&gt; next_time = sim.get_next_order_start_time()\n    &gt;&gt;&gt; print(next_time)\n    2024-01-15 12:30:45.123000+00:00\n    \"\"\"\n    time_ms = self._sim_cpp.getNextOrderStartTimeMs()\n\n    # Handle case when no next order exists (minimum int64 value)\n    if time_ms &lt;= -sys.maxsize:\n        return pd.NaT\n\n    # Convert milliseconds since epoch to UTC timestamp\n    return pd.Timestamp(time_ms, unit='ms', tz='UTC')\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.get_transactions","title":"<code>get_transactions()</code>","text":"<p>Retrieve all transactions that have occurred since the last call and clear the internal transaction log.</p> <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing all transactions that occurred, with the following columns: - timestamp: The UTC timestamp when the transaction occurred. - delivery_hour: The UTC timestamp of the delivery hour for the traded product. - price: The execution price of the transaction (EUR/MWh). - volume: The volume of the transaction (MW). - buy_order_type: The type of the buy order ('Market' or 'Limit'). - sell_order_type: The type of the sell order ('Market' or 'Limit'). - buy_order_id: The ID of the buy order. - sell_order_id: The ID of the sell order.</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def get_transactions(self):\n    \"\"\"\n    Retrieve all transactions that have occurred since the last call and clear the internal transaction log.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing all transactions that occurred, with the following columns:\n            - timestamp: The UTC timestamp when the transaction occurred.\n            - delivery_hour: The UTC timestamp of the delivery hour for the traded product.\n            - price: The execution price of the transaction (EUR/MWh).\n            - volume: The volume of the transaction (MW).\n            - buy_order_type: The type of the buy order ('Market' or 'Limit').\n            - sell_order_type: The type of the sell order ('Market' or 'Limit').\n            - buy_order_id: The ID of the buy order.\n            - sell_order_id: The ID of the sell order.\n    \"\"\"\n    transactions = self._sim_cpp.getTransactions()\n    transactions = pd.DataFrame(transactions)\n    if not transactions.empty:\n        transactions[\"timestamp\"] = pd.to_datetime(transactions[\"timestamp\"], utc=True)\n        transactions[\"delivery_hour\"] = pd.to_datetime(transactions[\"delivery_hour\"], utc=True)\n    return transactions\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.group_transactions","title":"<code>group_transactions(transactions)</code>","text":"<p>Group transactions by timestamp and delivery hour, calculating volume-weighted average prices.</p> <p>Parameters:</p> Name Type Description Default <code>transactions</code> <code>DataFrame</code> <p>A DataFrame containing the transactions to be grouped.</p> required Processing Steps <ul> <li>Group the transactions by timestamp and delivery_hour.</li> <li>Calculate the volume weighted average price for each group.</li> <li>Return a DataFrame with aggregated transaction data.</li> </ul> <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame with the following columns: - timestamp: The UTC timestamp when the transaction occurred. - delivery_hour: The UTC timestamp of the delivery hour for the traded product. - vwap: The volume weighted average price of the transaction. - total_volume: The total volume of the transaction. - num_transactions: The number of transactions in the group.</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def group_transactions(self, transactions: pd.DataFrame):\n    \"\"\"\n    Group transactions by timestamp and delivery hour, calculating volume-weighted average prices.\n\n    Args:\n        transactions (pd.DataFrame): A DataFrame containing the transactions to be grouped.\n\n    Processing Steps:\n        - Group the transactions by timestamp and delivery_hour.\n        - Calculate the volume weighted average price for each group.\n        - Return a DataFrame with aggregated transaction data.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the following columns:\n            - timestamp: The UTC timestamp when the transaction occurred.\n            - delivery_hour: The UTC timestamp of the delivery hour for the traded product.\n            - vwap: The volume weighted average price of the transaction.\n            - total_volume: The total volume of the transaction.\n            - num_transactions: The number of transactions in the group.\n    \"\"\"\n\n    vwap_results = []\n\n    # Group by timestamp and delivery_hour\n    grouped = transactions.groupby(['timestamp', 'delivery_hour'])\n\n    for (timestamp, delivery_hour), group in grouped:\n        if len(group) == 1:\n            # Single transaction - use price and volume directly\n            row = group.iloc[0]\n            vwap = row['price']\n            total_volume = row['volume']\n        else:\n            # Multiple transactions - calculate volume weighted average price\n            total_volume = group['volume'].sum()\n            weighted_price_sum = (group['price'] * group['volume']).sum()\n            vwap = weighted_price_sum / total_volume if total_volume &gt; 0 else 0\n\n        vwap_results.append({\n            'timestamp': timestamp,\n            'delivery_hour': delivery_hour,\n            'vwap': vwap,\n            'total_volume': total_volume,\n            'num_transactions': len(group)\n        })\n\n    if vwap_results:\n        vwap_df = pd.DataFrame(vwap_results)\n    else:\n        vwap_df = pd.DataFrame()\n\n    return vwap_df\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.has_orders_remaining","title":"<code>has_orders_remaining()</code>","text":"<p>Check if there are remaining orders in the order queue.</p>"},{"location":"simulation/#bitepy.Simulation.has_orders_remaining--returns","title":"Returns","text":"<p>bool     True if there are remaining orders in the queue, False otherwise.</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def has_orders_remaining(self) -&gt; bool:\n    \"\"\"\n    Check if there are remaining orders in the order queue.\n\n    Returns\n    -------\n    bool\n        True if there are remaining orders in the queue, False otherwise.\n    \"\"\"\n    return self._sim_cpp.hasOrdersRemaining()\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.has_stopped_at_stop_time","title":"<code>has_stopped_at_stop_time()</code>","text":"<p>Check if the simulation has stopped due to the stop time being reached. Is set to false again once we set a new stop time.</p>"},{"location":"simulation/#bitepy.Simulation.has_stopped_at_stop_time--returns","title":"Returns","text":"<p>bool     True if the simulation stopped because the last order's submission time     exceeded the set stop time. False otherwise.</p>"},{"location":"simulation/#bitepy.Simulation.has_stopped_at_stop_time--notes","title":"Notes","text":"<ul> <li>This flag is set when the simulation stops due to a stop time set via set_stop_time()</li> <li>The flag is automatically reset when a new stop time is set</li> <li>Use this to determine if a simulation pause was due to the stop time condition</li> </ul>"},{"location":"simulation/#bitepy.Simulation.has_stopped_at_stop_time--example","title":"Example","text":"<p>sim.set_stop_time(pd.Timestamp('2024-01-15 12:30:00', tz='UTC')) sim.run_one_day(is_last=False) if sim.has_stopped_at_stop_time(): ...     print(\"Simulation stopped at the specified stop time\")</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def has_stopped_at_stop_time(self) -&gt; bool:\n    \"\"\"\n    Check if the simulation has stopped due to the stop time being reached. Is set to false again once we set a new stop time.\n\n    Returns\n    -------\n    bool\n        True if the simulation stopped because the last order's submission time\n        exceeded the set stop time. False otherwise.\n\n    Notes\n    -----\n    - This flag is set when the simulation stops due to a stop time set via set_stop_time()\n    - The flag is automatically reset when a new stop time is set\n    - Use this to determine if a simulation pause was due to the stop time condition\n\n    Example\n    -------\n    &gt;&gt;&gt; sim.set_stop_time(pd.Timestamp('2024-01-15 12:30:00', tz='UTC'))\n    &gt;&gt;&gt; sim.run_one_day(is_last=False)\n    &gt;&gt;&gt; if sim.has_stopped_at_stop_time():\n    ...     print(\"Simulation stopped at the specified stop time\")\n    \"\"\"\n    return self._sim_cpp.hasStoppedAtStopTime()\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.print_parameters","title":"<code>print_parameters()</code>","text":"<p>Print the simulation parameters, including start/end times, storage settings, and various limits and costs.</p> Processing Steps <ul> <li>Extract simulation start, end, and trading start times from internal parameters.</li> <li>Display all relevant storage configuration parameters.</li> <li>Show trading and technical constraints.</li> </ul> Source code in <code>bitepy/simulation.py</code> <pre><code>def print_parameters(self):\n    \"\"\"\n    Print the simulation parameters, including start/end times, storage settings, and various limits and costs.\n\n    Processing Steps:\n        - Extract simulation start, end, and trading start times from internal parameters.\n        - Display all relevant storage configuration parameters.\n        - Show trading and technical constraints.\n    \"\"\"\n    startMonth = self._sim_cpp.params.startMonth\n    startDay = self._sim_cpp.params.startDay\n    startYear = self._sim_cpp.params.startYear\n    startHour = self._sim_cpp.params.startHour\n    startMinute = self._sim_cpp.params.startMinute\n    endMonth = self._sim_cpp.params.endMonth\n    endDay = self._sim_cpp.params.endDay\n    endYear = self._sim_cpp.params.endYear\n    endHour = self._sim_cpp.params.endHour\n    endMinute = self._sim_cpp.params.endMinute\n    tradingStartMonth = self._sim_cpp.params.tradingStartMonth\n    tradingStartDay = self._sim_cpp.params.tradingStartDay\n    tradingStartYear = self._sim_cpp.params.tradingStartYear\n    tradingStartHour = self._sim_cpp.params.tradingStartHour\n    tradingStartMinute = self._sim_cpp.params.tradingStartMinute\n    cycleLimit = self._sim_cpp.params.cycleLimit\n\n    startDate = pd.Timestamp(year=startYear, month=startMonth, day=startDay, hour=startHour, minute=startMinute, tz=\"UTC\")\n    endDate = pd.Timestamp(year=endYear, month=endMonth, day=endDay, hour=endHour, minute=endMinute, tz=\"UTC\")\n    tradingStartDate = pd.Timestamp(year=tradingStartYear, month=tradingStartMonth, day=tradingStartDay, hour=tradingStartHour, minute=tradingStartMinute, tz=\"UTC\")\n\n    print(\"Start Time (UTC):\", startDate)\n    print(\"End Time (UTC):\", endDate)\n    print(\"Trading Start Time (UTC):\", tradingStartDate)\n\n    print(\"Storage Maximum:\", self._sim_cpp.params.storageMax, \"MWh\")\n    print(\"Linear Degredation Cost:\", self._sim_cpp.params.linDegCost, \"\u20ac/MWh\")\n    print(\"Injection Loss \u03b7+:\", self._sim_cpp.params.lossIn)\n    print(\"Withdrawal Loss \u03b7-:\", self._sim_cpp.params.lossOut)\n    print(\"Trading Fee:\", self._sim_cpp.params.tradingFee, \"\u20ac/MWh\")\n    print(\"Number of DP Storage States:\", self._sim_cpp.params.numStorStates)\n    print(\"Technical Delay:\", self._sim_cpp.params.pingDelay, \"ms\")\n    print(\"Trading Delay:\", self._sim_cpp.params.minuteDelay, \"min\")\n    print(\"Fixed Solve Time:\", self._sim_cpp.params.fixedSolveTime, \"ms\")\n    print(\"Solve Frequency:\", self._sim_cpp.params.dpFreq, \"min\")\n    print(\"Injection Maximum:\", self._sim_cpp.params.injectMax, \"MW\")\n    print(\"Withdrawal Maximum:\", self._sim_cpp.params.withdrawMax, \"MW\")\n    print(\"Log Transactions:\", self._sim_cpp.params.logTransactions)\n    print(\"Only Traverse LOB:\", self._sim_cpp.params.onlyTraverseLOB)\n    print(\"Cycle Limit:\", cycleLimit)\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.reached_end_of_day","title":"<code>reached_end_of_day(is_last)</code>","text":"<p>Check if the order queue has reached the end for this day.</p> <p>This function mirrors the logic of run_one_day(is_last) for checking whether we've processed all available orders in the current batch.</p>"},{"location":"simulation/#bitepy.Simulation.reached_end_of_day--parameters","title":"Parameters","text":"<p>is_last : bool     Whether this is the last data batch (same semantics as run_one_day).     - If False: indicates more data-days will be loaded after this batch     - If True: indicates this is the final batch of data-days for this simulation</p>"},{"location":"simulation/#bitepy.Simulation.reached_end_of_day--returns","title":"Returns","text":"<p>bool     True if there are no more orders to process in the queue, False otherwise.</p>"},{"location":"simulation/#bitepy.Simulation.reached_end_of_day--notes","title":"Notes","text":"<ul> <li>Returns True when orderQueue.hasNext() is False in C++</li> <li>The is_last parameter is kept for API consistency with run_one_day</li> <li>Use this to check if the simulation stopped because it ran out of orders   vs. stopping due to a stop time or stop hour</li> </ul>"},{"location":"simulation/#bitepy.Simulation.reached_end_of_day--example","title":"Example","text":"<p>sim.run_one_day(is_last=False) if sim.reached_end_of_day(is_last=False): ...     print(\"Processed all orders in current batch, ready for next day's data\") elif sim.has_stopped_at_stop_time(): ...     print(\"Stopped at specified stop time\")</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def reached_end_of_day(self, is_last: bool) -&gt; bool:\n    \"\"\"\n    Check if the order queue has reached the end for this day.\n\n    This function mirrors the logic of run_one_day(is_last) for checking\n    whether we've processed all available orders in the current batch.\n\n    Parameters\n    ----------\n    is_last : bool\n        Whether this is the last data batch (same semantics as run_one_day).\n        - If False: indicates more data-days will be loaded after this batch\n        - If True: indicates this is the final batch of data-days for this simulation\n\n    Returns\n    -------\n    bool\n        True if there are no more orders to process in the queue, False otherwise.\n\n    Notes\n    -----\n    - Returns True when orderQueue.hasNext() is False in C++\n    - The is_last parameter is kept for API consistency with run_one_day\n    - Use this to check if the simulation stopped because it ran out of orders\n      vs. stopping due to a stop time or stop hour\n\n    Example\n    -------\n    &gt;&gt;&gt; sim.run_one_day(is_last=False)\n    &gt;&gt;&gt; if sim.reached_end_of_day(is_last=False):\n    ...     print(\"Processed all orders in current batch, ready for next day's data\")\n    &gt;&gt;&gt; elif sim.has_stopped_at_stop_time():\n    ...     print(\"Stopped at specified stop time\")\n    \"\"\"\n    return self._sim_cpp.reachedEndOfDay(is_last)\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.return_vol_price_pairs","title":"<code>return_vol_price_pairs(is_last, frequency, volumes)</code>","text":"<p>Retrieve volume-price pairs from the simulation.</p> <p>Parameters:</p> Name Type Description Default <code>is_last</code> <code>bool</code> <p>If True, indicates this is the last iteration of data.</p> required <code>frequency</code> <code>int</code> <p>The frequency (in seconds) at which price data is retrieved.</p> required <code>volumes</code> <code>ndarray</code> <p>A 1D numpy array of volumes for which prices are returned.</p> required Processing Steps <ul> <li>Validate input parameters for correct format and values.</li> <li>Extract volume-price data from the simulation at specified frequency.</li> <li>Convert timestamps to UTC format for consistency.</li> </ul> <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame with columns: - current_time: Time of the export (UTC). - delivery_hour: Delivery period time (UTC). - volume: The volume for which the price is exported (MWh). - price_full: The full price (cashflow) for the volume (\u20ac). - worst_accepted_price: Market price of the worst matched order (\u20ac/MWh).</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def return_vol_price_pairs(self, is_last: bool, frequency: int, volumes: np.ndarray):\n    \"\"\"\n    Retrieve volume-price pairs from the simulation.\n\n    Args:\n        is_last (bool): If True, indicates this is the last iteration of data.\n        frequency (int): The frequency (in seconds) at which price data is retrieved.\n        volumes (np.ndarray): A 1D numpy array of volumes for which prices are returned.\n\n    Processing Steps:\n        - Validate input parameters for correct format and values.\n        - Extract volume-price data from the simulation at specified frequency.\n        - Convert timestamps to UTC format for consistency.\n\n    Returns:\n        pd.DataFrame: A DataFrame with columns:\n            - current_time: Time of the export (UTC).\n            - delivery_hour: Delivery period time (UTC).\n            - volume: The volume for which the price is exported (MWh).\n            - price_full: The full price (cashflow) for the volume (\u20ac).\n            - worst_accepted_price: Market price of the worst matched order (\u20ac/MWh).\n    \"\"\"\n    if len(volumes.shape) != 1:\n        raise ValueError(\"volumes must be a 1D numpy array\")\n    if frequency &lt;= 0:\n        raise ValueError(\"frequency must be &gt; 0\")\n\n    vol_price_list = self._sim_cpp.return_vol_price_pairs(is_last, frequency, volumes)\n    vol_price_list = pd.DataFrame(vol_price_list)\n\n    if not vol_price_list.empty:\n        vol_price_list[\"current_time\"] = pd.to_datetime(vol_price_list[\"current_time\"], utc=True)\n        vol_price_list[\"delivery_hour\"] = pd.to_datetime(vol_price_list[\"delivery_hour\"], utc=True)\n\n    return vol_price_list\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.run","title":"<code>run(data_path, verbose=True)</code>","text":"<p>Execute the simulation using binary data files.</p> <p>The files must be named as: orderbook_YYYY-MM-DD.bin.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The directory containing the binary data files.</p> required <code>verbose</code> <code>bool</code> <p>If True, display progress logs. Default is True.</p> <code>True</code> Processing Steps <ul> <li>Retrieve the list of binary file paths for the simulation period.</li> <li>Iterate through each day's data, add the file to the order queue, and run the simulation for that day.</li> </ul> <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing the transactions if log_transactions is True, otherwise None.</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def run(self, data_path: str, verbose: bool = True):\n    \"\"\"\n    Execute the simulation using binary data files.\n\n    The files must be named as: orderbook_YYYY-MM-DD.bin.\n\n    Args:\n        data_path (str): The directory containing the binary data files.\n        verbose (bool, optional): If True, display progress logs. Default is True.\n\n    Processing Steps:\n        - Retrieve the list of binary file paths for the simulation period.\n        - Iterate through each day's data, add the file to the order queue, and run the simulation for that day.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the transactions if log_transactions is True, otherwise None.\n\n    \"\"\"\n    start_date = pd.Timestamp(year=self._sim_cpp.params.startYear,\n                              month=self._sim_cpp.params.startMonth,\n                              day=self._sim_cpp.params.startDay,\n                              hour=self._sim_cpp.params.startHour,\n                              minute=self._sim_cpp.params.startMinute,\n                              tz=\"UTC\")\n    end_date = pd.Timestamp(year=self._sim_cpp.params.endYear,\n                            month=self._sim_cpp.params.endMonth,\n                            day=self._sim_cpp.params.endDay,\n                            hour=self._sim_cpp.params.endHour,\n                            minute=self._sim_cpp.params.endMinute,\n                            tz=\"UTC\")\n    lob_paths = self.get_data_bins_for_each_day(data_path, start_date, end_date)\n\n    transactions = pd.DataFrame()\n\n    num_days = len(lob_paths)\n    if verbose: print(\"The simulation will iterate over\", num_days, \"files.\")\n\n    with tqdm(total=num_days, desc=\"Simulated Days\", unit=\"%\", ncols=120, disable=not verbose) as pbar:\n        for i, path in enumerate(lob_paths):\n            pbar.set_description(f\"Currently simulating {path.split('/')[-1]} ... \")\n            self.add_bin_to_orderqueue(path)\n            self.run_one_day(i == len(lob_paths) - 1)\n            if self._sim_cpp.params.logTransactions:\n                transactions = pd.concat([transactions, self.group_transactions(self.get_transactions())])\n            pbar.update(1)\n\n    if verbose: print(\"Simulation finished.\")\n\n    if self._sim_cpp.params.logTransactions and not transactions.empty:\n        return transactions\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.run_one_day","title":"<code>run_one_day(is_last)</code>","text":"<p>Run the simulation for a single day.</p> <p>Parameters:</p> Name Type Description Default <code>is_last</code> <code>bool</code> <p>If True, indicates that this is the last iteration of data.</p> required Processing Steps <ul> <li>Execute the simulation for the provided day's data.</li> </ul> Source code in <code>bitepy/simulation.py</code> <pre><code>def run_one_day(self, is_last: bool):\n    \"\"\"\n    Run the simulation for a single day.\n\n    Args:\n        is_last (bool): If True, indicates that this is the last iteration of data.\n\n    Processing Steps:\n        - Execute the simulation for the provided day's data.\n    \"\"\"\n    self._sim_cpp.run(is_last)\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.set_stop_time","title":"<code>set_stop_time(stop_time, verbose=False)</code>","text":"<p>Set a datetime with millisecond precision to stop the simulation once.</p> <p>The simulation will stop only once, if the last order added has a submission time after the stop time. Once the simulation has stopped, the stop time is automatically cleared, allowing you to set a new one.</p>"},{"location":"simulation/#bitepy.Simulation.set_stop_time--parameters","title":"Parameters","text":"<p>stop_time : pd.Timestamp     A timezone-aware timestamp with millisecond precision when the simulation should stop.     The simulation will stop if the last processed order's submission time is &gt; this stop time. verbose : bool, optional     Whether to print a message when the simulation stops. Raises</p> <p>ValueError     If stop_time is not timezone aware.</p>"},{"location":"simulation/#bitepy.Simulation.set_stop_time--notes","title":"Notes","text":"<ul> <li>The stop time is checked after each order is processed</li> <li>The simulation stops only once per stop time setting</li> <li>After stopping, the stop time is automatically cleared</li> <li>You can set a new stop time after the simulation has stopped</li> <li>The stop time is compared against the order's submission time (transaction time)</li> </ul> Source code in <code>bitepy/simulation.py</code> <pre><code>def set_stop_time(self, stop_time: pd.Timestamp, verbose: bool = False):\n    \"\"\"\n    Set a datetime with millisecond precision to stop the simulation once.\n\n    The simulation will stop only once, if the last order added has a submission time\n    after the stop time. Once the simulation has stopped, the stop time is automatically\n    cleared, allowing you to set a new one.\n\n    Parameters\n    ----------\n    stop_time : pd.Timestamp\n        A timezone-aware timestamp with millisecond precision when the simulation should stop.\n        The simulation will stop if the last processed order's submission time is &gt; this stop time.\n    verbose : bool, optional\n        Whether to print a message when the simulation stops.\n    Raises\n    ------\n    ValueError\n        If stop_time is not timezone aware.\n\n    Notes\n    -----\n    - The stop time is checked after each order is processed\n    - The simulation stops only once per stop time setting\n    - After stopping, the stop time is automatically cleared\n    - You can set a new stop time after the simulation has stopped\n    - The stop time is compared against the order's submission time (transaction time)\n\n    \"\"\"\n    # Check if timezone aware\n    if stop_time.tzinfo is None:\n        raise ValueError(\"stop_time must be timezone aware\")\n\n    # Convert to UTC\n    stop_time_utc = stop_time.astimezone(pytz.utc)\n\n    # Convert to milliseconds since epoch\n    stop_time_ms = int(stop_time_utc.timestamp() * 1000)\n\n    # Call C++ method\n    self._sim_cpp.setStopTime(stop_time_ms, verbose)\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.solve","title":"<code>solve()</code>","text":"<p>Solve the dynamic programming problem once using the time of the last placed order.</p>"},{"location":"simulation/#bitepy.Simulation.solve--returns","title":"Returns","text":"<p>pd.DataFrame     A DataFrame containing the suggested to be executed market orders from the solve.     Columns include: dp_run, time, last_solve_time, hour, reward,     reward_incl_deg_costs, volume, type, final_pos, final_stor.</p>"},{"location":"simulation/#bitepy.Simulation.solve--notes","title":"Notes","text":"<p>This function calls the C++ solve() method once. It does not run the full simulation, only performs a single DP solve at the time of the last placed order. If no orders have been placed yet, the behavior depends on the initial state of _lastOrder_placementTime.</p>"},{"location":"simulation/#bitepy.Simulation.solve--example","title":"Example","text":"<p>orders_df = sim.solve()</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def solve(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Solve the dynamic programming problem once using the time of the last placed order.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame containing the suggested to be executed market orders from the solve.\n        Columns include: dp_run, time, last_solve_time, hour, reward,\n        reward_incl_deg_costs, volume, type, final_pos, final_stor.\n\n    Notes\n    -----\n    This function calls the C++ solve() method once. It does not run the\n    full simulation, only performs a single DP solve at the time of the\n    last placed order. If no orders have been placed yet, the behavior\n    depends on the initial state of _lastOrder_placementTime.\n\n    Example\n    -------\n    &gt;&gt;&gt; orders_df = sim.solve()\n    \"\"\"\n    # Call C++ method (no parameters needed - uses last order placement time)\n    order_list = self._sim_cpp.solve()\n\n    # Convert list of dicts to pandas DataFrame\n    if not order_list:\n        # Return empty DataFrame with expected columns\n        return pd.DataFrame(columns=[\n            'dp_run', 'time', 'last_solve_time', 'hour', 'reward',\n            'reward_incl_deg_costs', 'volume', 'type', 'final_pos', 'final_stor'\n        ])\n\n    order_df = pd.DataFrame(order_list)\n    order_df['time'] = pd.to_datetime(order_df['time'], utc=True)\n    order_df['last_solve_time'] = pd.to_datetime(order_df['last_solve_time'], utc=True)\n    order_df['hour'] = pd.to_datetime(order_df['hour'], utc=True)\n\n    return order_df\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.submit_limit_orders","title":"<code>submit_limit_orders(df)</code>","text":"<p>Submit a list of limit orders and track their matches without battery optimization.</p> <p>This method validates input data and queues the limit orders for submission at specified times. The orders will be submitted during the normal simulation run without triggering battery optimization.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame containing the limit orders to be submitted. The DataFrame must have the following columns:     - transaction_time: The time when the order should be submitted (timezone aware, up to millisecond precision).     - price: The price of the limit order (\u20ac/MWh).     - volume: The volume of the limit order (MWh, positive for buy, negative for sell).     - side: The side of the order ('buy' or 'sell').     - delivery_time: The delivery time for the order (timezone aware, required).</p> required Processing Steps <ul> <li>Validates input data format and required columns.</li> <li>Ensures timezone awareness and proper formatting of timestamps.</li> <li>Queues the limit orders for submission during simulation execution.</li> <li>Orders are processed without triggering battery optimization.</li> </ul> <p>Returns:</p> Name Type Description <code>None</code> <p>This method queues orders but does not return match information. After running the simulation, use get_limit_order_matches() to retrieve match details.</p> Note <p>Call this method to queue own limit orders, then run the simulation to process them and collect matches. Use get_limit_order_matches() after simulation to retrieve final match results.</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def submit_limit_orders(self, df: pd.DataFrame):\n    \"\"\"\n    Submit a list of limit orders and track their matches without battery optimization.\n\n    This method validates input data and queues the limit orders for submission at specified times.\n    The orders will be submitted during the normal simulation run without triggering battery optimization.\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing the limit orders to be submitted.\n            The DataFrame must have the following columns:\n                - transaction_time: The time when the order should be submitted (timezone aware, up to millisecond precision).\n                - price: The price of the limit order (\u20ac/MWh).\n                - volume: The volume of the limit order (MWh, positive for buy, negative for sell).\n                - side: The side of the order ('buy' or 'sell').\n                - delivery_time: The delivery time for the order (timezone aware, required).\n\n    Processing Steps:\n        - Validates input data format and required columns.\n        - Ensures timezone awareness and proper formatting of timestamps.\n        - Queues the limit orders for submission during simulation execution.\n        - Orders are processed without triggering battery optimization.\n\n    Returns:\n        None: This method queues orders but does not return match information.\n            After running the simulation, use get_limit_order_matches() to retrieve match details.\n\n    Note:\n        Call this method to queue own limit orders, then run the simulation to process them and collect matches.\n        Use get_limit_order_matches() after simulation to retrieve final match results.\n    \"\"\"\n\n    # Validate input DataFrame\n    required_columns = ['transaction_time', 'price', 'volume', 'side', 'delivery_time']\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"Missing required columns: {missing_columns}\")\n\n    # Check if transaction_time is timezone aware\n    if df[\"transaction_time\"].dt.tz is None:\n        raise ValueError(\"transaction_time must be timezone aware\")\n    if df[\"transaction_time\"].isna().any():\n        raise ValueError(\"transaction_time cannot contain NaT values - all transaction times are required\")\n\n    # Check if delivery_time is timezone aware and has no NaT values\n    if df[\"delivery_time\"].dt.tz is None:\n        raise ValueError(\"delivery_time must be timezone aware\")\n    if df[\"delivery_time\"].isna().any():\n        raise ValueError(\"delivery_time cannot contain NaT values - all delivery times are required\")\n\n    # check that the delivery time is a full hour exactly\n    if (df[\"delivery_time\"].dt.minute != 0).any():\n        raise ValueError(\"delivery_time must be a full hour exactly for hourly products\")\n\n    # volume must be &gt; 0\n    if df[\"volume\"].le(0).any():\n        raise ValueError(\"volume must be &gt; 0\")\n\n    # Convert to UTC\n    df = df.copy()\n    df[\"transaction_time\"] = df[\"transaction_time\"].dt.tz_convert(\"UTC\")\n    df[\"delivery_time\"] = df[\"delivery_time\"].dt.tz_convert(\"UTC\")\n\n    # Validate side column\n    valid_sides = {'buy', 'sell', 'Buy', 'Sell', 'BUY', 'SELL'}\n    invalid_sides = df[\"side\"].unique()\n    invalid_sides = [side for side in invalid_sides if side not in valid_sides]\n    if invalid_sides:\n        raise ValueError(f\"Invalid side values: {invalid_sides}. Must be one of: {valid_sides}\")\n\n    # Normalize side values to 'Buy'/'Sell'\n    df[\"side\"] = df[\"side\"].str.capitalize()\n\n    # Convert timestamps to ISO format\n    df[\"transaction_time\"] = df[\"transaction_time\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n    df[\"delivery_time\"] = df[\"delivery_time\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n\n    # Prepare data for C++ function\n    transaction_times = df[\"transaction_time\"].to_numpy(dtype='str').tolist()\n    prices = df[\"price\"].to_numpy(dtype=np.float64).tolist()\n    volumes = df[\"volume\"].to_numpy(dtype=np.float64).tolist()\n    sides = df[\"side\"].to_numpy(dtype='str').tolist()\n    delivery_times = df[\"delivery_time\"].to_numpy(dtype='str').tolist()\n\n    # Call C++ function to submit limit orders (no return value)\n    self._sim_cpp.submitLimitOrdersAndGetMatches(transaction_times, prices, volumes, sides, delivery_times)\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.transform_lob_to_levels","title":"<code>transform_lob_to_levels(lob_state_df, exchange='EPEX', product_name='XBID_Hour_Power', delivery_area='10YDE-VE-------2', product_duration_hours=1)</code>","text":"<p>Transforms the output of get_limit_order_book_state (individual orders) into an aggregated, price-level-based DataFrame.</p>"},{"location":"simulation/#bitepy.Simulation.transform_lob_to_levels--parameters","title":"Parameters","text":"<p>lob_state_df : pd.DataFrame     The DataFrame returned by get_limit_order_book_state.     Must contain columns: 'delivery_time', 'side', 'price', 'volume'. exchange : str, optional     Static value for the 'exchange' column in the output. product_name : str, optional     Static value for the 'product' column in the output. delivery_area : str, optional     Static value for the 'deliveryArea' column in the output. product_duration_hours : int, optional     Duration of the product in hours, used to calculate     deliveryEndUtc from delivery_time (which is used as deliveryStartUtc).     Default is 1.</p>"},{"location":"simulation/#bitepy.Simulation.transform_lob_to_levels--returns","title":"Returns","text":"<p>pd.DataFrame     A DataFrame in the target format with aggregated price levels and columns:     ['exchange', 'product', 'deliveryStartUtc', 'deliveryEndUtc',     'deliveryArea', 'side', 'level', 'price', 'quantity']</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def transform_lob_to_levels(\n    self,\n    lob_state_df: pd.DataFrame,\n    exchange: str = \"EPEX\",\n    product_name: str = \"XBID_Hour_Power\",\n    delivery_area: str = \"10YDE-VE-------2\",\n    product_duration_hours: int = 1\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transforms the output of get_limit_order_book_state (individual orders)\n    into an aggregated, price-level-based DataFrame.\n\n    Parameters\n    ----------\n    lob_state_df : pd.DataFrame\n        The DataFrame returned by get_limit_order_book_state.\n        Must contain columns: 'delivery_time', 'side', 'price', 'volume'.\n    exchange : str, optional\n        Static value for the 'exchange' column in the output.\n    product_name : str, optional\n        Static value for the 'product' column in the output.\n    delivery_area : str, optional\n        Static value for the 'deliveryArea' column in the output.\n    product_duration_hours : int, optional\n        Duration of the product in hours, used to calculate\n        deliveryEndUtc from delivery_time (which is used as deliveryStartUtc).\n        Default is 1.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame in the target format with aggregated price levels and columns:\n        ['exchange', 'product', 'deliveryStartUtc', 'deliveryEndUtc',\n        'deliveryArea', 'side', 'level', 'price', 'quantity']\n    \"\"\"\n\n    # Define the target column order\n    target_columns = [\n        'exchange', 'product', 'deliveryStartUtc', 'deliveryEndUtc',\n        'deliveryArea', 'side', 'level', 'price', 'quantity'\n    ]\n\n    # Keep only the columns we need from the start\n    source_columns = ['delivery_time', 'side', 'price', 'volume']\n\n    # Handle empty input DataFrame\n    if lob_state_df.empty:\n        return pd.DataFrame(columns=target_columns)\n\n    all_orders_dfs = []\n    product_duration = pd.Timedelta(hours=product_duration_hours)\n\n    # Group by each product (identified by its delivery_time)\n    for delivery_time, product_df in lob_state_df[source_columns].groupby('delivery_time'):\n\n        delivery_start_utc = delivery_time\n        delivery_end_utc = delivery_start_utc + product_duration\n\n        # --- Process Bids (Source 'buy' side) ---\n        buys_df = product_df[product_df['side'] == 'buy']\n        if not buys_df.empty:\n            # We assume buys_df is already sorted by price (descending)\n            bids_1_to_1 = buys_df.copy().reset_index(drop=True)\n\n            # Set level as the order rank\n            bids_1_to_1['level'] = bids_1_to_1.index\n            bids_1_to_1['side'] = 'bid'\n            bids_1_to_1 = bids_1_to_1.rename(columns={'volume': 'quantity'})\n\n            # Add common metadata\n            bids_1_to_1['exchange'] = exchange\n            bids_1_to_1['product'] = product_name\n            bids_1_to_1['deliveryStartUtc'] = delivery_start_utc\n            bids_1_to_1['deliveryEndUtc'] = delivery_end_utc\n            bids_1_to_1['deliveryArea'] = delivery_area\n\n            all_orders_dfs.append(bids_1_to_1[target_columns]) # Select only target cols\n\n        # --- Process Asks (Source 'sell' side) ---\n        sells_df = product_df[product_df['side'] == 'sell']\n        if not sells_df.empty:\n            # We assume sells_df is already sorted by price (ascending)\n            asks_1_to_1 = sells_df.copy().reset_index(drop=True)\n\n            # Set level as the order rank\n            asks_1_to_1['level'] = asks_1_to_1.index\n            asks_1_to_1['side'] = 'ask'\n            asks_1_to_1 = asks_1_to_1.rename(columns={'volume': 'quantity'})\n\n            # Add common metadata\n            asks_1_to_1['exchange'] = exchange\n            asks_1_to_1['product'] = product_name\n            asks_1_to_1['deliveryStartUtc'] = delivery_start_utc\n            asks_1_to_1['deliveryEndUtc'] = delivery_end_utc\n            asks_1_to_1['deliveryArea'] = delivery_area\n\n            all_orders_dfs.append(asks_1_to_1[target_columns]) # Select only target cols\n\n    # Handle case where input was not empty but contained no orders\n    if not all_orders_dfs:\n        return pd.DataFrame(columns=target_columns)\n\n    # Combine all products and sides into one DataFrame\n    final_df = pd.concat(all_orders_dfs, ignore_index=True)\n\n    return final_df\n</code></pre>"}]}